Issue Type,Summary,Description,Epic Link,Priority,Components,Labels,Story Points,Assignee,Reporter,Due Date
Story,Create Cost Monitoring and Alerts,"**OBJECTIVE:** Implement comprehensive cost monitoring, budgeting, and alerting system for CloudFlare AWS migration to ensure cost optimization and prevent budget overruns while maintaining operational visibility. **COST MONITORING ARCHITECTURE:** **Cost Management Components:** 1. **AWS Cost Explorer Integration** - Detailed cost analysis and forecasting 2. **Budget Management** - Proactive budget alerts and controls 3. **Resource Tagging Strategy** - Cost allocation and tracking 4. **Cost Optimization Recommendations** - Automated cost-saving opportunities 5. **Billing Alerts & Notifications** - Multi-level alerting system **DETAILED IMPLEMENTATION:** **1. Comprehensive Tagging Strategy:** ```yaml # Resource Tagging Standards required_tags: Project: ""CloudFlare-Migration"" Environment: ""Production"" # or Development, Staging Owner: ""Data-Engineering-Team"" CostCenter: ""Engineering"" Application: ""CloudFlare-Processing"" DataClassification: ""Internal"" BackupRequired: ""Yes"" # or No AutoShutdown: ""No"" # or Yes for non-production optional_tags: CreatedBy: ""${aws:username}"" CreatedDate: ""${aws:RequestedRegion}"" LastModified: ""Auto-Updated"" Purpose: ""Firewall-Log-Processing"" # Tagging enforcement policy tagging_policy: enforcement_level: ""strict"" # Require all required tags auto_tag_resources: - EC2 - EBS - S3 - CloudWatch - Lambda tag_compliance_check: enabled: true frequency: ""daily"" non_compliance_action: ""alert"" # or ""stop"" for strict enforcement ``` **2. AWS Budgets Configuration:** ```json { ""BudgetName"": ""CloudFlare-Processing-Monthly"", ""BudgetLimit"": { ""Amount"": ""2000"", ""Unit"": ""USD"" }, ""TimeUnit"": ""MONTHLY"", ""TimePeriod"": { ""Start"": ""2025-01-01"", ""End"": ""2026-12-31"" }, ""BudgetType"": ""COST"", ""CostFilters"": { ""TagKey"": [""Project""], ""TagValue"": [""CloudFlare-Migration""] }, ""NotificationsWithSubscribers"": [ { ""Notification"": { ""NotificationType"": ""ACTUAL"", ""ComparisonOperator"": ""GREATER_THAN"", ""Threshold"": 80, ""ThresholdType"": ""PERCENTAGE"" }, ""Subscribers"": [ { ""SubscriptionType"": ""EMAIL"", ""Address"": ""finance-team@company.com"" }, { ""SubscriptionType"": ""EMAIL"", ""Address"": ""engineering-lead@company.com"" } ] }, { ""Notification"": { ""NotificationType"": ""ACTUAL"", ""ComparisonOperator"": ""GREATER_THAN"", ""Threshold"": 95, ""ThresholdType"": ""PERCENTAGE"" }, ""Subscribers"": [ { ""SubscriptionType"": ""EMAIL"", ""Address"": ""cto@company.com"" }, { ""SubscriptionType"": ""SNS"", ""Address"": ""arn:aws:sns:us-east-1:813388701013:cost-alerts-critical"" } ] }, { ""Notification"": { ""NotificationType"": ""FORECASTED"", ""ComparisonOperator"": ""GREATER_THAN"", ""Threshold"": 100, ""ThresholdType"": ""PERCENTAGE"" }, ""Subscribers"": [ { ""SubscriptionType"": ""EMAIL"", ""Address"": ""finance-team@company.com"" } ] } ] } ``` **3. Cost Monitoring Dashboard:** ```python # cost_monitor.py import boto3 from datetime import datetime, timedelta import json import structlog logger = structlog.get_logger() class CostMonitor: def __init__(self): self.ce_client = boto3.client('ce') # Cost Explorer self.cloudwatch = boto3.client('cloudwatch') self.sns = boto3.client('sns') def get_daily_costs(self, days_back: int = 30) -> dict: \"\"\"Get daily costs for CloudFlare processing resources.\"\"\" end_date = datetime.now().date() start_date = end_date - timedelta(days=days_back) try: response = self.ce_client.get_cost_and_usage( TimePeriod={ 'Start': start_date.strftime('%Y-%m-%d'), 'End': end_date.strftime('%Y-%m-%d') }, Granularity='DAILY', Metrics=['BlendedCost', 'UsageQuantity'], GroupBy=[ {'Type': 'DIMENSION', 'Key': 'SERVICE'}, {'Type': 'TAG', 'Key': 'Project'} ], Filter={ 'Tags': { 'Key': 'Project', 'Values': ['CloudFlare-Migration'] } } ) return self._process_cost_data(response) except Exception as e: logger.error(f""Failed to retrieve cost data: {e}"") return {} def get_service_breakdown(self) -> dict: \"\"\"Get cost breakdown by AWS service.\"\"\" end_date = datetime.now().date() start_date = end_date - timedelta(days=7) # Last 7 days try: response = self.ce_client.get_cost_and_usage( TimePeriod={ 'Start': start_date.strftime('%Y-%m-%d'), 'End': end_date.strftime('%Y-%m-%d') }, Granularity='DAILY', Metrics=['BlendedCost'], GroupBy=[{'Type': 'DIMENSION', 'Key': 'SERVICE'}], Filter={ 'Tags': { 'Key': 'Project', 'Values': ['CloudFlare-Migration'] } } ) service_costs = {} for result in response['ResultsByTime']: for group in result['Groups']: service = group['Keys'][0] cost = float(group['Metrics']['BlendedCost']['Amount']) if service in service_costs: service_costs[service] += cost else: service_costs[service] = cost return service_costs except Exception as e: logger.error(f""Failed to get service breakdown: {e}"") return {} def get_cost_forecast(self, days_ahead: int = 30) -> dict: \"\"\"Get cost forecast for next 30 days.\"\"\" start_date = datetime.now().date() end_date = start_date + timedelta(days=days_ahead) try: response = self.ce_client.get_cost_forecast( TimePeriod={ 'Start': start_date.strftime('%Y-%m-%d'), 'End': end_date.strftime('%Y-%m-%d') }, Metric='BLENDED_COST', Granularity='MONTHLY', Filter={ 'Tags': { 'Key': 'Project', 'Values': ['CloudFlare-Migration'] } } ) return { 'forecasted_cost': float(response['Total']['Amount']), 'forecast_confidence': response['ForecastConfidenceLevel'] } except Exception as e: logger.error(f""Failed to get cost forecast: {e}"") return {} def check_budget_alerts(self): \"\"\"Check current spend against budget and send alerts if needed.\"\"\" current_month_cost = self._get_current_month_cost() monthly_budget = 2000.0 # $2000 monthly budget spend_percentage = (current_month_cost / monthly_budget) * 100 # Send CloudWatch metric self.cloudwatch.put_metric_data( Namespace='CloudFlare/Cost', MetricData=[ { 'MetricName': 'MonthlySpendPercentage', 'Value': spend_percentage, 'Unit': 'Percent' }, { 'MetricName': 'MonthlySpendAmount', 'Value': current_month_cost, 'Unit': 'None' } ] ) # Check alert thresholds if spend_percentage >= 95: self._send_critical_cost_alert(current_month_cost, monthly_budget) elif spend_percentage >= 80: self._send_warning_cost_alert(current_month_cost, monthly_budget) return { 'current_spend': current_month_cost, 'budget': monthly_budget, 'percentage': spend_percentage } def generate_cost_optimization_report(self) -> dict: \"\"\"Generate cost optimization recommendations.\"\"\" recommendations = [] # Check for unused resources recommendations.extend(self._check_unused_ebs_volumes()) recommendations.extend(self._check_underutilized_instances()) recommendations.extend(self._check_s3_storage_optimization()) recommendations.extend(self._check_reserved_instance_opportunities()) return { 'timestamp': datetime.now().isoformat(), 'total_recommendations': len(recommendations), 'potential_monthly_savings': sum(r.get('monthly_savings', 0) for r in recommendations), 'recommendations': recommendations } def _check_unused_ebs_volumes(self) -> list: \"\"\"Check for unattached EBS volumes.\"\"\" ec2 = boto3.client('ec2') recommendations = [] try: volumes = ec2.describe_volumes( Filters=[ {'Name': 'status', 'Values': ['available']}, {'Name': 'tag:Project', 'Values': ['CloudFlare-Migration']} ] ) for volume in volumes['Volumes']: size_gb = volume['Size'] monthly_cost = size_gb * 0.10 # Approximate cost per GB recommendations.append({ 'type': 'unused_ebs_volume', 'resource_id': volume['VolumeId'], 'description': f""Unattached EBS volume ({size_gb}GB)"", 'monthly_savings': monthly_cost, 'action': 'Delete if not needed' }) except Exception as e: logger.error(f""Failed to check EBS volumes: {e}"") return recommendations def _send_critical_cost_alert(self, current_cost: float, budget: float): \"\"\"Send critical cost alert.\"\"\" message = f\"\"\" CRITICAL COST ALERT: CloudFlare Processing Current monthly spend: ${current_cost:.2f} Monthly budget: ${budget:.2f} Percentage: {(current_cost/budget)*100:.1f}% Action required: Review and optimize resources immediately. \"\"\" try: self.sns.publish( TopicArn='arn:aws:sns:us-east-1:813388701013:cost-alerts-critical', Subject='CRITICAL: CloudFlare Processing Budget Alert', Message=message ) logger.warning(f""Sent critical cost alert: ${current_cost:.2f}"") except Exception as e: logger.error(f""Failed to send cost alert: {e}"") ``` **4. Cost Optimization Automation:** ```bash #!/bin/bash # cost_optimization.sh # Automated cost optimization checks and actions set -euo pipefail LOG_FILE=""/data/logs/cost_optimization_$(date +%Y%m%d).log"" exec 1> >(tee -a ""$LOG_FILE"") exec 2>&1 echo ""[$(date)] Starting cost optimization checks"" # Function to check and stop idle instances check_idle_instances() { echo ""Checking for idle EC2 instances..."" # Get instances with low CPU utilization aws cloudwatch get-metric-statistics \ --namespace AWS/EC2 \ --metric-name CPUUtilization \ --dimensions Name=InstanceId,Value=i-1234567890abcdef0 \ --start-time $(date -d '24 hours ago' -u +%Y-%m-%dT%H:%M:%S) \ --end-time $(date -u +%Y-%m-%dT%H:%M:%S) \ --period 3600 \ --statistics Average \ --query 'Datapoints[?Average<`5.0`]' > /tmp/idle_instances.json if [ -s /tmp/idle_instances.json ]; then echo ""Found idle instances - sending alert"" aws sns publish \ --topic-arn ""arn:aws:sns:us-east-1:813388701013:cost-optimization"" \ --message ""Idle EC2 instances detected for CloudFlare processing. Consider stopping or right-sizing."" fi } # Function to optimize S3 storage optimize_s3_storage() { echo ""Optimizing S3 storage..."" # Check for objects that can be moved to cheaper storage classes aws s3api list-objects-v2 \ --bucket cars-cloudflare-processed \ --query 'Contents[?LastModified<`2025-01-01`]' > /tmp/old_objects.json if [ -s /tmp/old_objects.json ]; then echo ""Found objects eligible for storage class transition"" # Apply lifecycle policies automatically aws s3api put-bucket-lifecycle-configuration \ --bucket cars-cloudflare-processed \ --lifecycle-configuration file://s3_lifecycle_policy.json fi } # Function to check Reserved Instance opportunities check_ri_opportunities() { echo ""Checking Reserved Instance opportunities..."" # Get current instance usage patterns aws ce get-reservation-utilization \ --time-period Start=$(date -d '30 days ago' +%Y-%m-%d),End=$(date +%Y-%m-%d) \ --group-by Type=DIMENSION,Key=SERVICE \ --filter file://ri_filter.json > /tmp/ri_analysis.json # Analyze and recommend RIs python3 -c "" import json with open('/tmp/ri_analysis.json') as f: data = json.load(f) # Simple RI recommendation logic for result in data.get('UtilizationsByTime', []): utilization = float(result['Total']['UtilizationPercentage']) if utilization > 75: print('Recommend Reserved Instances for consistent usage') "" } # Main execution check_idle_instances optimize_s3_storage check_ri_opportunities echo ""[$(date)] Cost optimization checks completed"" ``` **5. Cost Reporting Dashboard:** ```json { ""widgets"": [ { ""type"": ""metric"", ""properties"": { ""metrics"": [ [ ""CloudFlare/Cost"", ""MonthlySpendAmount"" ] ], ""period"": 86400, ""stat"": ""Average"", ""region"": ""us-east-1"", ""title"": ""Monthly Spend Tracking"" } }, { ""type"": ""metric"", ""properties"": { ""metrics"": [ [ ""AWS/Billing"", ""EstimatedCharges"", ""Currency"", ""USD"", ""ServiceName"", ""AmazonEC2"" ], [ ""..."", ""AmazonS3"" ], [ ""..."", ""AmazonCloudWatch"" ] ], ""period"": 86400, ""stat"": ""Maximum"", ""region"": ""us-east-1"", ""title"": ""Service Cost Breakdown"" } }, { ""type"": ""log"", ""properties"": { ""query"": ""SOURCE '/aws/cost/optimization' | fields @timestamp, recommendation, potential_savings | sort @timestamp desc"", ""region"": ""us-east-1"", ""title"": ""Cost Optimization Opportunities"" } } ] } ``` **ACCEPTANCE CRITERIA:** - [ ] AWS Cost Explorer integration configured - [ ] Monthly budget created with multi-level alerts (80%, 95%, 100%) - [ ] Comprehensive resource tagging strategy implemented - [ ] Cost monitoring dashboard created with key metrics - [ ] Automated cost optimization checks implemented - [ ] Reserved Instance opportunity analysis - [ ] S3 storage optimization automation - [ ] Idle resource detection and alerting - [ ] Cost forecasting and trend analysis - [ ] SNS integration for cost alerts - [ ] CloudWatch custom metrics for cost tracking - [ ] Weekly cost optimization reports - [ ] Budget variance analysis and reporting **BUDGET TARGETS:** **Phase 1 (Month 1-2): Foundation** - Budget: $500-1000/month - Primary costs: EC2 instance, EBS storage, S3 storage - Alert thresholds: 80% ($400-800), 95% ($475-950) **Phase 2 (Month 3-4): Scale Processing** - Budget: $1000-3000/month - Additional costs: Data transfer, CloudWatch, processing scale-up - Alert thresholds: 80% ($800-2400), 95% ($950-2850) **Phase 3 (Month 5+): Production Scale** - Budget: $2000-5000/month - Full production workload with optimization - Alert thresholds: 80% ($1600-4000), 95% ($1900-4750) **COST OPTIMIZATION STRATEGIES:** 1. **EC2 Optimization:** - Right-size instances based on actual usage - Use Spot Instances for non-critical processing (60-70% savings) - Implement auto-shutdown for development instances 2. **S3 Optimization:** - Implement Intelligent Tiering - Use lifecycle policies for data archival - Optimize multipart upload settings 3. **Data Transfer Optimization:** - Use S3 Transfer Acceleration judiciously - Optimize data processing to minimize cross-region transfers - Implement data compression where possible 4. **Reserved Instance Strategy:** - Purchase RIs for consistent baseline usage - Monitor utilization and adjust as needed **MONITORING AND ALERTING:** **Daily Monitoring:** - Cost trend analysis - Budget burn rate - Resource utilization metrics **Weekly Reports:** - Cost breakdown by service - Optimization opportunities - Budget variance analysis **Monthly Reviews:** - Budget performance - Cost optimization implementation - Forecast accuracy assessment",CloudFlare Data Processing Migration to AWS,Medium,Infrastructure,aws-cost-management,3,,,
Story,Perform Initial Data Migration Test,"**OBJECTIVE:** Execute comprehensive test migration with representative dataset to validate end-to-end processing pipeline, performance benchmarks, and data integrity before full-scale migration. **TEST MIGRATION SCOPE:** **Test Dataset Selection:** - **Primary Source**: 2.ford (Ford firewall events) - **Date Range**: 1 week of recent data (7 consecutive days) - **Expected Volume**: ~50-100 million records - **File Types**: Mixed .gz compressed files - **Estimated Size**: 2-5 GB compressed, 15-25 GB uncompressed **TEST OBJECTIVES:** 1. **Functional Validation** - End-to-end pipeline functionality 2. **Performance Benchmarking** - Processing speed and resource utilization 3. **Data Integrity Verification** - Completeness and accuracy validation 4. **Error Handling Testing** - Failure scenarios and recovery 5. **Monitoring Validation** - Metrics and alerting systems **DETAILED TEST PLAN:** **Phase 1: Pre-Test Setup and Validation** ```bash #!/bin/bash # test_migration_setup.sh # Pre-test environment validation and setup set -euo pipefail TEST_DIR=""/data/test_migration"" LOG_FILE=""$TEST_DIR/test_migration_$(date +%Y%m%d_%H%M%S).log"" SOURCE=""2.ford"" TEST_DATE_START=""20250115"" # Adjust to actual test date TEST_DATE_END=""20250121"" # 7 days of data # Setup test environment setup_test_environment() { echo ""[$(date)] Setting up test environment"" mkdir -p ""$TEST_DIR""/{input,output,logs,reports} # Verify AWS access echo ""Verifying AWS access..."" aws sts get-caller-identity --profile cars-platform-security aws s3 ls s3://cloudflare-logs-di/logs/firewall/di-websites/$SOURCE/ --profile di-security # Verify processing environment echo ""Verifying processing environment..."" cd /opt/cloudflare-processor uv --version python --version # Check available disk space AVAILABLE_SPACE=$(df /data | awk 'NR==2 {print $4}') REQUIRED_SPACE=52428800 # 50GB in KB if [ $AVAILABLE_SPACE -lt $REQUIRED_SPACE ]; then echo ""ERROR: Insufficient disk space. Available: ${AVAILABLE_SPACE}KB, Required: ${REQUIRED_SPACE}KB"" exit 1 fi echo ""Environment setup complete"" } # Identify test data identify_test_data() { echo ""[$(date)] Identifying test data"" local current_date=$TEST_DATE_START while [ ""$current_date"" != $(date -d ""$TEST_DATE_END + 1 day"" +%Y%m%d) ]; do echo ""Checking data for $current_date"" aws s3 ls ""s3://cloudflare-logs-di/logs/firewall/di-websites/$SOURCE/$current_date/"" \ --recursive --profile di-security > ""$TEST_DIR/available_files_$current_date.txt"" if [ -s ""$TEST_DIR/available_files_$current_date.txt"" ]; then echo ""✓ Data available for $current_date"" else echo ""⚠ No data found for $current_date"" fi current_date=$(date -d ""$current_date + 1 day"" +%Y%m%d) done # Calculate total test data size TOTAL_SIZE=$(cat $TEST_DIR/available_files_*.txt | awk '{sum += $3} END {print sum}') echo ""Total test data size: $(($TOTAL_SIZE / 1024 / 1024)) MB"" } ``` **Phase 2: Baseline Performance Measurement** ```python # performance_baseline.py import time import psutil import polars as pl from typing import Dict, Any import structlog logger = structlog.get_logger() class PerformanceMonitor: def __init__(self): self.start_time = None self.baseline_metrics = {} def start_monitoring(self): \"\"\"Start performance monitoring.\"\"\" self.start_time = time.time() self.baseline_metrics = { 'cpu_percent': psutil.cpu_percent(interval=1), 'memory_percent': psutil.virtual_memory().percent, 'disk_usage': psutil.disk_usage('/data').percent, 'network_io_start': psutil.net_io_counters() } logger.info(f""Started performance monitoring: {self.baseline_metrics}"") def get_current_metrics(self) -> Dict[str, Any]: \"\"\"Get current system metrics.\"\"\" current_time = time.time() elapsed_time = current_time - self.start_time if self.start_time else 0 current_net_io = psutil.net_io_counters() baseline_net_io = self.baseline_metrics.get('network_io_start') return { 'elapsed_time': elapsed_time, 'cpu_percent': psutil.cpu_percent(), 'memory_percent': psutil.virtual_memory().percent, 'memory_used_gb': psutil.virtual_memory().used / (1024**3), 'disk_usage_percent': psutil.disk_usage('/data').percent, 'network_bytes_sent': current_net_io.bytes_sent - baseline_net_io.bytes_sent if baseline_net_io else 0, 'network_bytes_recv': current_net_io.bytes_recv - baseline_net_io.bytes_recv if baseline_net_io else 0 } def benchmark_processing_performance(self, test_file: str) -> Dict[str, Any]: \"\"\"Benchmark processing performance with test data.\"\"\" logger.info(f""Starting performance benchmark with {test_file}"") self.start_monitoring() # Load test data start_load = time.time() df = pl.read_parquet(test_file) load_time = time.time() - start_load # Process data start_process = time.time() # Simulate processing operations processed_df = ( df .with_columns([ pl.col('Datetime').dt.hour().alias('Hour'), pl.col('ClientIP').str.lengths().alias('IP_Length'), pl.when(pl.col('Action') == 'BLOCK') .then(10) .otherwise(1) .alias('RiskScore') ]) .filter(pl.col('ClientIP').is_not_null()) .group_by(['Hour', 'Action']) .agg([pl.count().alias('Count')]) ) process_time = time.time() - start_process # Export results start_export = time.time() output_file = test_file.replace('.parquet', '_processed.parquet') processed_df.write_parquet(output_file) export_time = time.time() - start_export total_time = load_time + process_time + export_time records_per_second = len(df) / total_time if total_time > 0 else 0 final_metrics = self.get_current_metrics() return { 'input_records': len(df), 'output_records': len(processed_df), 'load_time': load_time, 'process_time': process_time, 'export_time': export_time, 'total_time': total_time, 'records_per_second': records_per_second, 'peak_memory_gb': final_metrics['memory_used_gb'], 'network_transfer_mb': (final_metrics['network_bytes_sent'] + final_metrics['network_bytes_recv']) / (1024**2) } ``` **Phase 3: End-to-End Pipeline Test** ```bash #!/bin/bash # end_to_end_test.sh # Complete end-to-end pipeline test set -euo pipefail TEST_SOURCE=""2.ford"" TEST_DATE=""20250115"" # Single day for initial test PROJECT_DIR=""/opt/cloudflare-processor"" TEST_OUTPUT_DIR=""/data/test_migration/output"" # Execute full pipeline test execute_pipeline_test() { echo ""[$(date)] Starting end-to-end pipeline test"" local start_time=$(date +%s) # Step 1: Download test data echo ""Step 1: Downloading test data..."" local temp_dir=""/data/test_migration/input/$TEST_SOURCE/$TEST_DATE"" mkdir -p ""$temp_dir"" aws s3 sync ""s3://cloudflare-logs-di/logs/firewall/di-websites/$TEST_SOURCE/$TEST_DATE/"" ""$temp_dir/"" \ --profile di-security if [ $? -ne 0 ]; then echo ""ERROR: Failed to download test data"" return 1 fi # Step 2: Process data echo ""Step 2: Processing data..."" cd ""$PROJECT_DIR"" if uv run python -m cloudflare_processor.cli.main process \ --source ""$TEST_SOURCE"" \ --date ""$TEST_DATE"" \ --input-dir ""$temp_dir"" \ --output-dir ""$TEST_OUTPUT_DIR"" \ --config ""config/production.yaml""; then echo ""✓ Processing completed successfully"" else echo ""✗ Processing failed"" return 1 fi # Step 3: Validate output echo ""Step 3: Validating output..."" local output_file=""$TEST_OUTPUT_DIR/firewall_logs_${TEST_SOURCE}_${TEST_DATE}.parquet"" if [ -f ""$output_file"" ]; then # Get record count local record_count=$(uv run python -c ""import polars as pl; df = pl.read_parquet('$output_file'); print(len(df))"") echo ""✓ Output file created with $record_count records"" else echo ""✗ Output file not found"" return 1 fi # Step 4: Upload to S3 echo ""Step 4: Uploading to S3..."" aws s3 cp ""$output_file"" ""s3://cars-cloudflare-processed/test/processed/$TEST_SOURCE/"" \ --profile cars-platform-security aws s3 cp ""$TEST_OUTPUT_DIR/firewall_logs_${TEST_SOURCE}_${TEST_DATE}.metadata.json"" \ ""s3://cars-cloudflare-processed/test/processed/$TEST_SOURCE/"" \ --profile cars-platform-security # Step 5: Run validation echo ""Step 5: Running data validation..."" uv run python -m cloudflare_processor.cli.validate \ --source ""$TEST_SOURCE"" \ --date ""$TEST_DATE"" \ --input-file ""$output_file"" \ --config ""config/validation_config.yaml"" \ --output-dir ""/data/test_migration/validation"" local end_time=$(date +%s) local duration=$((end_time - start_time)) echo ""[$(date)] End-to-end test completed in ${duration} seconds"" # Generate test report generate_test_report ""$duration"" ""$record_count"" } ``` **Phase 4: Performance Benchmarking** ```python # performance_test.py import time import concurrent.futures from pathlib import Path import polars as pl import structlog logger = structlog.get_logger() class PerformanceBenchmark: def __init__(self, test_dir: str): self.test_dir = Path(test_dir) self.results = {} def run_scalability_test(self): \"\"\"Test processing performance with different data sizes.\"\"\" test_sizes = [1, 5, 10, 25, 50] # Million records for size_mb in test_sizes: logger.info(f""Testing with ~{size_mb}M records"") # Create test dataset of specified size test_file = self._create_test_dataset(size_mb) # Run processing benchmark result = self._benchmark_processing(test_file) self.results[f""{size_mb}M_records""] = result logger.info(f""Completed {size_mb}M records test: {result['records_per_second']:.0f} records/sec"") def run_concurrency_test(self): \"\"\"Test concurrent processing performance.\"\"\" test_file = self._create_test_dataset(10) # 10M records thread_counts = [1, 2, 4, 8] for thread_count in thread_counts: logger.info(f""Testing with {thread_count} threads"") start_time = time.time() with concurrent.futures.ThreadPoolExecutor(max_workers=thread_count) as executor: futures = [] for i in range(thread_count): future = executor.submit(self._process_chunk, test_file, i) futures.append(future) # Wait for completion results = [future.result() for future in futures] end_time = time.time() total_time = end_time - start_time total_records = sum(r['records_processed'] for r in results) throughput = total_records / total_time self.results[f""{thread_count}_threads""] = { 'total_time': total_time, 'total_records': total_records, 'throughput': throughput, 'threads': thread_count } logger.info(f""Completed {thread_count} threads test: {throughput:.0f} records/sec"") def _benchmark_processing(self, test_file: Path) -> dict: \"\"\"Benchmark single-threaded processing.\"\"\" start_time = time.time() # Load data load_start = time.time() df = pl.read_parquet(test_file) load_time = time.time() - load_start # Process data process_start = time.time() processed_df = self._process_dataframe(df) process_time = time.time() - process_start total_time = time.time() - start_time return { 'input_records': len(df), 'output_records': len(processed_df), 'load_time': load_time, 'process_time': process_time, 'total_time': total_time, 'records_per_second': len(df) / total_time, 'memory_efficiency': len(processed_df) / len(df) } ``` **Phase 5: Data Integrity Validation** ```python # data_integrity_test.py import polars as pl from typing import Dict, List import hashlib import json class DataIntegrityValidator: def __init__(self): self.validation_results = [] def validate_data_integrity(self, source_files: List[str], processed_file: str) -> Dict: \"\"\"Comprehensive data integrity validation.\"\"\" # Load source data source_dfs = [] for file_path in source_files: if file_path.endswith('.gz'): df = pl.read_csv(file_path, compression='gzip') else: df = pl.read_csv(file_path) source_dfs.append(df) combined_source = pl.concat(source_dfs) # Load processed data processed_df = pl.read_parquet(processed_file) # Run integrity checks results = { 'record_count_check': self._check_record_counts(combined_source, processed_df), 'data_completeness_check': self._check_data_completeness(combined_source, processed_df), 'data_accuracy_check': self._check_data_accuracy(combined_source, processed_df), 'schema_consistency_check': self._check_schema_consistency(processed_df), 'duplicate_check': self._check_duplicates(processed_df) } # Calculate overall integrity score total_checks = len(results) passed_checks = sum(1 for result in results.values() if result['status'] == 'PASSED') integrity_score = (passed_checks / total_checks) * 100 return { 'overall_integrity_score': integrity_score, 'detailed_results': results, 'recommendation': 'PROCEED' if integrity_score >= 95 else 'INVESTIGATE' } def _check_record_counts(self, source_df: pl.DataFrame, processed_df: pl.DataFrame) -> Dict: \"\"\"Check if record counts are within acceptable variance.\"\"\" source_count = len(source_df) processed_count = len(processed_df) variance = abs(source_count - processed_count) / source_count * 100 # Allow up to 5% variance for data cleaning acceptable_variance = 5.0 status = 'PASSED' if variance <= acceptable_variance else 'FAILED' return { 'status': status, 'source_count': source_count, 'processed_count': processed_count, 'variance_percentage': variance, 'threshold': acceptable_variance } ``` **ACCEPTANCE CRITERIA:** - [ ] Test environment setup and validated - [ ] Representative test dataset identified and accessible - [ ] End-to-end pipeline test executed successfully - [ ] Performance benchmarks completed and documented - [ ] Data integrity validation passed (>95% integrity score) - [ ] Error handling scenarios tested - [ ] Monitoring and alerting validated during test - [ ] Resource utilization measured and optimized - [ ] Processing time meets performance targets (<2 hours for daily data) - [ ] Memory usage stays within instance limits - [ ] S3 transfer performance acceptable - [ ] Test results documented and reviewed - [ ] Go/no-go decision made for full migration **PERFORMANCE TARGETS:** - **Processing Speed**: >10,000 records/second - **Memory Usage**: <80% of available RAM - **Processing Time**: Complete 1 day of data in <2 hours - **Data Integrity**: >95% validation success rate - **Error Rate**: <1% processing failures **TEST SUCCESS CRITERIA:** 1. **Functional Success**: All pipeline components work end-to-end 2. **Performance Success**: Meets or exceeds performance targets 3. **Quality Success**: Data integrity validation passes 4. **Operational Success**: Monitoring and alerting function correctly 5. **Scalability Success**: Performance scales linearly with data size **DELIVERABLES:** - Test execution report with detailed results - Performance benchmark analysis - Data integrity validation report - Resource utilization analysis - Recommendations for full migration - Updated processing parameters based on test results",CloudFlare Data Processing Migration to AWS,High,Testing,migration-test,8,,,
Story,Execute Full Historical Data Migration,"**OBJECTIVE:** Migrate all historical CloudFlare firewall data (277 parquet files, 2+ billion events) from DI Security S3 bucket to Cars Platform Security account with full processing, validation, and performance optimization. **MIGRATION SCOPE:** **Complete Data Inventory:** - **Total Volume**: 2,041,081,108 firewall events - **File Count**: 277 parquet files - **Data Sources**: 2.ford (Ford), 3 (Unknown), 47 (Largest dataset) - **Date Range**: 2025-04-24 to 2025-07-16 (84 days) - **Estimated Size**: ~500GB compressed, ~2TB uncompressed - **Processing Target**: Complete within 2 weeks (14 days) **MIGRATION ARCHITECTURE:** **Parallel Processing Strategy:** 1. **Multi-Source Processing** - Process all 3 sources concurrently 2. **Date-Based Parallelization** - Process multiple dates simultaneously 3. **Intelligent Scheduling** - Prioritize recent data, optimize resource usage 4. **Checkpoint Recovery** - Resume from failures without data loss 5. **Progress Tracking** - Real-time migration status monitoring **DETAILED IMPLEMENTATION:** **1. Migration Orchestrator (migration_orchestrator.py):** ```python import asyncio import boto3 import polars as pl from concurrent.futures import ThreadPoolExecutor, as_completed from dataclasses import dataclass from typing import List, Dict, Optional import structlog from datetime import datetime, timedelta import json import time logger = structlog.get_logger() @dataclass class MigrationTask: source: str date: str priority: int # 1=highest, 5=lowest estimated_records: int status: str = 'pending' # pending, processing, completed, failed start_time: Optional[datetime] = None end_time: Optional[datetime] = None error_message: Optional[str] = None class HistoricalDataMigrator: def __init__(self, config: Dict): self.config = config self.s3_client = boto3.client('s3') self.cloudwatch = boto3.client('cloudwatch') self.migration_tasks: List[MigrationTask] = [] self.completed_tasks = 0 self.failed_tasks = 0 self.total_records_processed = 0 async def execute_full_migration(self): \"\"\"Execute complete historical data migration.\"\"\" logger.info(""Starting full historical data migration"") # Phase 1: Discovery and planning await self._discover_migration_tasks() await self._optimize_migration_plan() # Phase 2: Execute migration with parallel processing await self._execute_parallel_migration() # Phase 3: Validation and reporting await self._validate_migration_results() await self._generate_migration_report() logger.info(""Full historical data migration completed"") async def _discover_migration_tasks(self): \"\"\"Discover all data that needs to be migrated.\"\"\" logger.info(""Discovering migration tasks"") sources = ['2.ford', '3', '47'] start_date = datetime(2025, 4, 24) end_date = datetime(2025, 7, 16) current_date = start_date while current_date <= end_date: date_str = current_date.strftime('%Y%m%d') for source in sources: # Check if data exists in source bucket if await self._check_source_data_exists(source, date_str): # Check if already processed if not await self._check_already_processed(source, date_str): # Estimate record count estimated_records = await self._estimate_record_count(source, date_str) # Assign priority (recent data = higher priority) days_from_end = (end_date - current_date).days priority = min(5, max(1, days_from_end // 10)) task = MigrationTask( source=source, date=date_str, priority=priority, estimated_records=estimated_records ) self.migration_tasks.append(task) current_date += timedelta(days=1) logger.info(f""Discovered {len(self.migration_tasks)} migration tasks"") async def _optimize_migration_plan(self): \"\"\"Optimize migration execution plan.\"\"\" # Sort by priority (1=highest) and estimated size self.migration_tasks.sort(key=lambda x: (x.priority, -x.estimated_records)) # Group tasks for parallel execution self.task_batches = self._create_task_batches() logger.info(f""Created {len(self.task_batches)} task batches for parallel execution"") async def _execute_parallel_migration(self): \"\"\"Execute migration with parallel processing.\"\"\" max_concurrent_tasks = self.config.get('max_concurrent_tasks', 4) semaphore = asyncio.Semaphore(max_concurrent_tasks) async def process_task_with_semaphore(task): async with semaphore: return await self._process_migration_task(task) # Process all tasks with concurrency control tasks = [process_task_with_semaphore(task) for task in self.migration_tasks] # Process in batches to manage memory batch_size = 10 for i in range(0, len(tasks), batch_size): batch = tasks[i:i + batch_size] batch_results = await asyncio.gather(*batch, return_exceptions=True) # Process results for task, result in zip(self.migration_tasks[i:i + batch_size], batch_results): if isinstance(result, Exception): task.status = 'failed' task.error_message = str(result) self.failed_tasks += 1 logger.error(f""Task failed: {task.source}/{task.date}: {result}"") else: task.status = 'completed' self.completed_tasks += 1 self.total_records_processed += task.estimated_records logger.info(f""Batch {i//batch_size + 1} completed"") # Send progress metrics await self._send_progress_metrics() async def _process_migration_task(self, task: MigrationTask) -> Dict: \"\"\"Process individual migration task.\"\"\" logger.info(f""Processing {task.source}/{task.date}"") task.start_time = datetime.now() task.status = 'processing' try: # Step 1: Download source data temp_dir = f""/data/migration/temp/{task.source}/{task.date}"" await self._download_source_data(task.source, task.date, temp_dir) # Step 2: Process data output_dir = f""/data/migration/processed/{task.source}"" processing_result = await self._process_data( task.source, task.date, temp_dir, output_dir ) # Step 3: Validate processed data validation_result = await self._validate_processed_data( task.source, task.date, processing_result['output_file'] ) # Step 4: Upload to target S3 bucket await self._upload_processed_data( task.source, task.date, processing_result['output_file'] ) # Step 5: Cleanup temporary data await self._cleanup_temp_data(temp_dir) task.end_time = datetime.now() duration = (task.end_time - task.start_time).total_seconds() logger.info(f""Completed {task.source}/{task.date} in {duration:.1f}s"") return { 'task': task, 'processing_result': processing_result, 'validation_result': validation_result, 'duration': duration } except Exception as e: task.status = 'failed' task.error_message = str(e) task.end_time = datetime.now() logger.error(f""Failed to process {task.source}/{task.date}: {e}"") raise async def _download_source_data(self, source: str, date: str, temp_dir: str): \"\"\"Download source data from DI Security S3 bucket.\"\"\" import os os.makedirs(temp_dir, exist_ok=True) # Use AWS CLI for efficient parallel downloads cmd = f\"\"\" aws s3 sync s3://cloudflare-logs-di/logs/firewall/di-websites/{source}/{date}/ {temp_dir}/ --profile di-security --quiet \"\"\" process = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() if process.returncode != 0: raise Exception(f""S3 download failed: {stderr.decode()}"") async def _process_data(self, source: str, date: str, input_dir: str, output_dir: str) -> Dict: \"\"\"Process raw data into parquet format.\"\"\" import os os.makedirs(output_dir, exist_ok=True) # Use the existing processing pipeline cmd = f\"\"\" cd /opt/cloudflare-processor && uv run python -m cloudflare_processor.cli.main process --source {source} --date {date} --input-dir {input_dir} --output-dir {output_dir} --config config/production.yaml \"\"\" process = await asyncio.create_subprocess_shell( cmd, stdout=asyncio.subprocess.PIPE, stderr=asyncio.subprocess.PIPE ) stdout, stderr = await process.communicate() if process.returncode != 0: raise Exception(f""Data processing failed: {stderr.decode()}"") output_file = f""{output_dir}/firewall_logs_{source}_{date}.parquet"" # Get record count for validation df = pl.read_parquet(output_file) record_count = len(df) return { 'output_file': output_file, 'record_count': record_count, 'file_size_mb': os.path.getsize(output_file) / (1024 * 1024) } async def _send_progress_metrics(self): \"\"\"Send migration progress metrics to CloudWatch.\"\"\" total_tasks = len(self.migration_tasks) completion_percentage = (self.completed_tasks / total_tasks) * 100 if total_tasks > 0 else 0 metrics = [ { 'MetricName': 'MigrationProgress', 'Value': completion_percentage, 'Unit': 'Percent' }, { 'MetricName': 'CompletedTasks', 'Value': self.completed_tasks, 'Unit': 'Count' }, { 'MetricName': 'FailedTasks', 'Value': self.failed_tasks, 'Unit': 'Count' }, { 'MetricName': 'TotalRecordsProcessed', 'Value': self.total_records_processed, 'Unit': 'Count' } ] try: self.cloudwatch.put_metric_data( Namespace='CloudFlare/Migration', MetricData=metrics ) except Exception as e: logger.error(f""Failed to send metrics: {e}"") ``` **2. Migration Progress Dashboard:** ```json { ""widgets"": [ { ""type"": ""metric"", ""properties"": { ""metrics"": [ [ ""CloudFlare/Migration"", ""MigrationProgress"" ] ], ""period"": 300, ""stat"": ""Average"", ""region"": ""us-east-1"", ""title"": ""Migration Progress (%)"", ""yAxis"": { ""left"": { ""min"": 0, ""max"": 100 } } } }, { ""type"": ""metric"", ""properties"": { ""metrics"": [ [ ""CloudFlare/Migration"", ""CompletedTasks"" ], [ ""."", ""FailedTasks"" ] ], ""period"": 300, ""stat"": ""Average"", ""region"": ""us-east-1"", ""title"": ""Task Completion Status"" } }, { ""type"": ""metric"", ""properties"": { ""metrics"": [ [ ""CloudFlare/Migration"", ""TotalRecordsProcessed"" ] ], ""period"": 3600, ""stat"": ""Average"", ""region"": ""us-east-1"", ""title"": ""Records Processed (Cumulative)"" } }, { ""type"": ""log"", ""properties"": { ""query"": ""SOURCE 'cloudflare-migration' | fields @timestamp, source, date, status, duration | filter status = 'completed' | sort @timestamp desc | limit 50"", ""region"": ""us-east-1"", ""title"": ""Recent Completed Migrations"" } } ] } ``` **3. Migration Recovery and Resumption:** ```python # migration_recovery.py class MigrationRecoveryManager: def __init__(self, state_file: str = '/data/migration/migration_state.json'): self.state_file = state_file def save_migration_state(self, migrator: HistoricalDataMigrator): \"\"\"Save current migration state for recovery.\"\"\" state = { 'timestamp': datetime.now().isoformat(), 'completed_tasks': migrator.completed_tasks, 'failed_tasks': migrator.failed_tasks, 'total_records_processed': migrator.total_records_processed, 'tasks': [ { 'source': task.source, 'date': task.date, 'status': task.status, 'start_time': task.start_time.isoformat() if task.start_time else None, 'end_time': task.end_time.isoformat() if task.end_time else None, 'error_message': task.error_message } for task in migrator.migration_tasks ] } with open(self.state_file, 'w') as f: json.dump(state, f, indent=2) def load_migration_state(self) -> Optional[Dict]: \"\"\"Load previous migration state for recovery.\"\"\" try: with open(self.state_file, 'r') as f: return json.load(f) except FileNotFoundError: return None def resume_migration(self, migrator: HistoricalDataMigrator) -> bool: \"\"\"Resume migration from previous state.\"\"\" state = self.load_migration_state() if not state: return False # Restore migration state migrator.completed_tasks = state['completed_tasks'] migrator.failed_tasks = state['failed_tasks'] migrator.total_records_processed = state['total_records_processed'] # Filter out completed tasks migrator.migration_tasks = [ task for task in migrator.migration_tasks if not any( t['source'] == task.source and t['date'] == task.date and t['status'] == 'completed' for t in state['tasks'] ) ] logger.info(f""Resumed migration with {len(migrator.migration_tasks)} remaining tasks"") return True ``` **4. Migration Validation and Quality Assurance:** ```python # migration_qa.py class MigrationQualityAssurance: def __init__(self): self.validation_results = {} async def validate_complete_migration(self, expected_tasks: List[MigrationTask]) -> Dict: \"\"\"Comprehensive validation of complete migration.\"\"\" validation_results = { 'data_completeness': await self._validate_data_completeness(expected_tasks), 'data_integrity': await self._validate_data_integrity(), 'performance_metrics': await self._analyze_performance_metrics(), 'cost_analysis': await self._analyze_migration_costs(), 'quality_score': 0 } # Calculate overall quality score scores = [] for category, result in validation_results.items(): if isinstance(result, dict) and 'score' in result: scores.append(result['score']) validation_results['quality_score'] = sum(scores) / len(scores) if scores else 0 return validation_results async def _validate_data_completeness(self, expected_tasks: List[MigrationTask]) -> Dict: \"\"\"Validate that all expected data has been migrated.\"\"\" completed_tasks = [task for task in expected_tasks if task.status == 'completed'] missing_tasks = [task for task in expected_tasks if task.status != 'completed'] completeness_percentage = (len(completed_tasks) / len(expected_tasks)) * 100 return { 'score': completeness_percentage, 'total_expected': len(expected_tasks), 'completed': len(completed_tasks), 'missing': len(missing_tasks), 'missing_details': [ {'source': task.source, 'date': task.date, 'status': task.status} for task in missing_tasks ] } ``` **ACCEPTANCE CRITERIA:** - [ ] All 277 parquet files successfully migrated and processed - [ ] Data integrity validation passes for all migrated data (>99% accuracy) - [ ] Migration completed within 14-day target timeline - [ ] Performance metrics meet or exceed targets - [ ] Zero data loss during migration process - [ ] All source data sources (2.ford, 3, 47) fully migrated - [ ] Processed data uploaded to Cars Platform Security S3 bucket - [ ] Migration progress tracking and reporting functional - [ ] Error handling and recovery mechanisms tested - [ ] Cost tracking and optimization during migration - [ ] Final validation report generated and approved - [ ] Migration state properly documented for future reference **PERFORMANCE TARGETS:** - **Daily Throughput**: Process 20+ days of historical data per day - **Processing Speed**: >15,000 records/second sustained - **Parallel Processing**: 4-8 concurrent migration tasks - **Error Rate**: <1% task failure rate - **Recovery Time**: <30 minutes to resume from failures **MIGRATION TIMELINE:** **Week 1 (Days 1-7):** - Days 1-2: Setup and initial batch processing - Days 3-5: Peak processing (50+ days of data) - Days 6-7: Quality assurance and validation **Week 2 (Days 8-14):** - Days 8-10: Complete remaining data processing - Days 11-12: Final validation and integrity checks - Days 13-14: Migration report and cleanup **RISK MITIGATION:** 1. **Data Loss Prevention:** - Checkpoint saves every 100 processed files - Source data remains untouched during migration - Validation at every step 2. **Performance Degradation:** - Dynamic scaling of concurrent tasks - Resource monitoring and optimization - Intelligent retry mechanisms 3. **Cost Overruns:** - Daily cost monitoring and alerts - Resource optimization during processing - Spot instance utilization where possible **SUCCESS METRICS:** - **Data Completeness**: 100% of expected data migrated - **Data Accuracy**: >99% validation success rate - **Timeline Adherence**: Complete within 14 days - **Cost Efficiency**: Stay within $5000 migration budget - **Zero Downtime**: No impact on source systems",CloudFlare Data Processing Migration to AWS,High,Development,full-migration,21,,,

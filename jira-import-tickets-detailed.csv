Issue Type,Summary,Description,Epic Link,Priority,Components,Labels,Story Points,Assignee,Reporter,Due Date
Epic,CloudFlare Data Processing Migration to AWS,"**BUSINESS CONTEXT:** Migrate CloudFlare firewall log processing from local infrastructure to AWS Cars Platform Security account for enterprise-scale data processing. **CURRENT STATE:** - 2+ billion firewall events processed locally with Polars/Python - 277 parquet files (~14GB memory usage) - Processing on single machine with performance limitations - Data sources: 2.ford (Ford), 3 (Unknown), 47 (Largest dataset) **TARGET STATE:** - AWS-based distributed processing pipeline - 10-100x performance improvement - Unlimited horizontal scaling capability - Pay-per-use cost model **BUSINESS VALUE:** - Process petabytes of data vs current GB limitations - Real-time analytics capabilities - Enterprise-grade monitoring and alerting - Cost optimization through serverless architecture **SUCCESS METRICS:** - Processing time: <2 hours for daily data (currently 8+ hours) - Cost target: $500-5000/month based on usage - Data integrity: 100% validation success rate - Availability: 99.9% uptime SLA",,High,Infrastructure,aws-migration,21,,,
Story,Setup Cross-Account IAM Roles,"**OBJECTIVE:** Enable secure cross-account access between DI Security (253296570627) and Cars Platform Security (813388701013) accounts for CloudFlare data processing. **TECHNICAL REQUIREMENTS:** **DI Security Account (253296570627) - Source Account:** 1. Create IAM Role: CloudFlareDataReader 2. Trust Policy: Allow Cars Platform Security account to assume role 3. Permissions Policy: Read access to S3 bucket only **Cars Platform Security Account (813388701013) - Target Account:** 1. Create IAM Role: CloudFlareDataProcessor 2. Permissions: Assume role in DI Security + full S3 access to new bucket 3. Attach to EC2 instance profile **DETAILED IMPLEMENTATION:** **Step 1: DI Security Account Role Creation** ```json { ""Version"": ""2012-10-17"", ""Statement"": [{ ""Effect"": ""Allow"", ""Principal"": { ""AWS"": ""arn:aws:iam::813388701013:role/CloudFlareDataProcessor"" }, ""Action"": ""sts:AssumeRole"", ""Condition"": { ""StringEquals"": { ""sts:ExternalId"": ""cloudflare-migration-2025"" } } }] } ``` **Step 2: S3 Permissions Policy** ```json { ""Version"": ""2012-10-17"", ""Statement"": [{ ""Effect"": ""Allow"", ""Action"": [""s3:GetObject"", ""s3:ListBucket""], ""Resource"": [ ""arn:aws:s3:::cloudflare-logs-di"", ""arn:aws:s3:::cloudflare-logs-di/*"" ] }] } ``` **Step 3: Cars Platform Security Role** - Create role with EC2 trust policy - Attach AWS managed policy: AmazonS3FullAccess - Add inline policy for cross-account assume role **ACCEPTANCE CRITERIA:** - [ ] IAM role created in DI Security account (253296570627) - [ ] IAM role created in Cars Platform Security account (813388701013) - [ ] Cross-account assume role tested successfully - [ ] S3 bucket access validated from target account - [ ] External ID security implemented - [ ] Role policies follow least privilege principle - [ ] CloudTrail logging enabled for role assumptions **TESTING STEPS:** 1. Assume role from Cars Platform Security account 2. List objects in s3://cloudflare-logs-di/logs/firewall/di-websites/ 3. Download sample file to verify read permissions 4. Confirm no write permissions to source bucket **SECURITY CONSIDERATIONS:** - Use external ID for additional security - Enable CloudTrail for audit logging - Implement time-based access if needed - Regular access review process",CloudFlare Data Processing Migration to AWS,High,Security,aws-iam cross-account,5,,,
Story,Create S3 Bucket for Processed Data,"**OBJECTIVE:** Create enterprise-grade S3 bucket infrastructure in Cars Platform Security account for storing processed CloudFlare data with proper organization, security, and lifecycle management. **TECHNICAL SPECIFICATIONS:** **Bucket Configuration:** - Name: cars-cloudflare-processed - Region: us-east-1 (same as source data) - Storage Class: Standard (with intelligent tiering) - Encryption: AES-256 server-side encryption - Versioning: Enabled for data protection **DETAILED FOLDER STRUCTURE:** ``` cars-cloudflare-processed/ ├── raw/ # Temporary raw data cache │ ├── 2.ford/ │ │ └── YYYYMMDD/ # Date-based partitioning │ ├── 3/ │ │ └── YYYYMMDD/ │ └── 47/ │ └── YYYYMMDD/ ├── processed/ # Final parquet files │ ├── 2.ford/ │ │ ├── firewall_logs_2_ford_YYYYMMDD.parquet │ │ └── firewall_logs_2_ford_YYYYMMDD.metadata.json │ ├── 3/ │ └── 47/ ├── analytics/ # Analysis outputs │ ├── dashboards/ │ ├── reports/ │ └── aggregations/ │ ├── daily/ │ ├── weekly/ │ └── monthly/ └── logs/ # Processing and system logs ├── processing/ │ └── YYYY/MM/DD/ ├── errors/ └── audit/ ``` **LIFECYCLE POLICIES:** 1. **Raw Data (Temporary):** - Delete after 7 days (data processed to parquet) - Transition to IA after 1 day if not deleted 2. **Processed Data:** - Standard storage for 30 days - Transition to IA after 30 days - Transition to Glacier after 90 days - Transition to Deep Archive after 1 year 3. **Analytics Data:** - Standard storage for 90 days - Transition to IA after 90 days 4. **Logs:** - Standard for 30 days, IA after 30 days, delete after 1 year **SECURITY CONFIGURATION:** - Block all public access - Enable access logging to separate bucket - Enable CloudTrail data events - Implement bucket notifications for processing triggers **PERFORMANCE OPTIMIZATION:** - Enable Transfer Acceleration - Configure multipart upload thresholds - Set up CloudFront distribution for analytics access **COST OPTIMIZATION:** - Enable S3 Intelligent Tiering - Implement incomplete multipart upload cleanup - Set up cost allocation tags **ACCEPTANCE CRITERIA:** - [ ] S3 bucket created with specified name and region - [ ] Folder structure implemented as specified - [ ] Lifecycle policies configured and tested - [ ] Encryption enabled (AES-256) - [ ] Versioning enabled - [ ] Access logging configured - [ ] CloudTrail data events enabled - [ ] Bucket notifications configured - [ ] Cost allocation tags applied - [ ] Transfer acceleration enabled - [ ] Public access blocked - [ ] Bucket policy restricts access to authorized roles only **MONITORING SETUP:** - CloudWatch metrics for bucket operations - Alerts for unusual access patterns - Cost monitoring and budgets - Data transfer monitoring **TESTING CHECKLIST:** 1. Upload test file to each folder structure 2. Verify lifecycle policy transitions (accelerated testing) 3. Test cross-account access with IAM roles 4. Validate encryption at rest 5. Confirm access logging functionality 6. Test bucket notifications",CloudFlare Data Processing Migration to AWS,Medium,Infrastructure,aws-s3,3,,,
Story,Launch EC2 Processing Instance,"**OBJECTIVE:** Launch and configure high-performance EC2 instance optimized for large-scale data processing with Polars and Python in Cars Platform Security account. **INSTANCE SPECIFICATIONS:** **Primary Option: r6i.4xlarge** - vCPUs: 16 - Memory: 128 GiB - Network: Up to 12.5 Gbps - EBS Bandwidth: Up to 6.25 Gbps - Cost: ~$0.504/hour (~$365/month if running 24/7) **Alternative: r6i.8xlarge (for larger datasets)** - vCPUs: 32 - Memory: 256 GiB - Network: 25 Gbps - Cost: ~$1.008/hour (~$730/month) **STORAGE CONFIGURATION:** **Primary EBS Volume (Root):** - Type: gp3 - Size: 100 GB - IOPS: 3000 - Throughput: 125 MB/s **Data Processing Volume:** - Type: gp3 - Size: 1000 GB (1TB) - IOPS: 10000 - Throughput: 1000 MB/s - Mount point: /data **DETAILED SETUP SCRIPT:** ```bash #!/bin/bash # EC2 User Data Script - CloudFlare Processing Instance set -e # Update system echo ""Updating system packages..."" yum update -y # Install development tools yum groupinstall -y ""Development Tools"" yum install -y python3.11 python3.11-pip python3.11-devel git htop iotop # Install UV (Python package manager) echo ""Installing UV package manager..."" curl -LsSf https://astral.sh/uv/install.sh | sh source /root/.cargo/env # Make UV available system-wide ln -sf /root/.cargo/bin/uv /usr/local/bin/uv # Create processing user and directories useradd -m -s /bin/bash cloudflare-processor usermod -aG wheel cloudflare-processor # Setup data directories mkdir -p /data/{temp,processed,logs,scripts} chown -R cloudflare-processor:cloudflare-processor /data chmod 755 /data/{temp,processed,logs,scripts} # Format and mount additional EBS volume if [ -b /dev/xvdf ]; then mkfs.ext4 /dev/xvdf mount /dev/xvdf /data echo '/dev/xvdf /data ext4 defaults,nofail 0 2' >> /etc/fstab fi # Install CloudWatch agent wget https://s3.amazonaws.com/amazoncloudwatch-agent/amazon_linux/amd64/latest/amazon-cloudwatch-agent.rpm rpm -U ./amazon-cloudwatch-agent.rpm # Setup Python environment cd /opt sudo -u cloudflare-processor mkdir -p /opt/cloudflare-processor cd /opt/cloudflare-processor sudo -u cloudflare-processor uv init --python 3.11 # Install core dependencies sudo -u cloudflare-processor uv add polars pandas pyarrow boto3 s3fs psutil # Install monitoring and utility packages sudo -u cloudflare-processor uv add prometheus-client structlog click # Create basic project structure sudo -u cloudflare-processor mkdir -p {src,tests,config,logs} # Setup logging configuration cat > /opt/cloudflare-processor/config/logging.conf << 'EOF' [loggers] keys=root [handlers] keys=consoleHandler,fileHandler [formatters] keys=simpleFormatter [logger_root] level=INFO handlers=consoleHandler,fileHandler [handler_consoleHandler] class=StreamHandler level=INFO formatter=simpleFormatter args=(sys.stdout,) [handler_fileHandler] class=FileHandler level=INFO formatter=simpleFormatter args=('/data/logs/processing.log',) [formatter_simpleFormatter] format=%(asctime)s - %(name)s - %(levelname)s - %(message)s EOF # Setup system monitoring cat > /opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json << 'EOF' { ""metrics"": { ""namespace"": ""CloudFlare/Processing"", ""metrics_collected"": { ""cpu"": { ""measurement"": [""cpu_usage_idle"", ""cpu_usage_iowait"", ""cpu_usage_user"", ""cpu_usage_system""], ""metrics_collection_interval"": 60 }, ""disk"": { ""measurement"": [""used_percent""], ""metrics_collection_interval"": 60, ""resources"": [""*""] }, ""diskio"": { ""measurement"": [""io_time""], ""metrics_collection_interval"": 60, ""resources"": [""*""] }, ""mem"": { ""measurement"": [""mem_used_percent""], ""metrics_collection_interval"": 60 } } } } EOF # Start CloudWatch agent /opt/aws/amazon-cloudwatch-agent/bin/amazon-cloudwatch-agent-ctl -a fetch-config -m ec2 -c file:/opt/aws/amazon-cloudwatch-agent/etc/amazon-cloudwatch-agent.json -s # Setup automatic updates echo ""0 2 * * * root yum update -y"" >> /etc/crontab # Create processing status endpoint mkdir -p /var/www/html echo ""Processing Instance Ready - $(date)"" > /var/www/html/status.html # Install and start nginx for health checks yum install -y nginx systemctl enable nginx systemctl start nginx echo ""Setup complete - $(date)"" >> /var/log/setup.log ``` **SECURITY CONFIGURATION:** - Security Group: CloudFlare-Processing-SG - Inbound: SSH (22) from bastion hosts only - Outbound: HTTPS (443) for AWS API calls, HTTP (80) for package downloads - IAM Role: CloudFlareDataProcessor (from previous story) - Key Pair: cloudflare-processing-keypair **NETWORK CONFIGURATION:** - VPC: Default VPC in Cars Platform Security account - Subnet: Private subnet with NAT Gateway access - Elastic IP: Not required (private instance) - Enhanced Networking: Enabled **MONITORING AND ALERTING:** **CloudWatch Metrics:** - CPU Utilization (alert if >80% for 10 minutes) - Memory Utilization (alert if >85% for 5 minutes) - Disk Usage (alert if >90%) - Network I/O - EBS Volume metrics **Custom Application Metrics:** - Processing job duration - Records processed per minute - Error rates - Queue depth **ACCEPTANCE CRITERIA:** - [ ] EC2 instance launched with specified configuration - [ ] Additional EBS volume attached and mounted - [ ] UV package manager installed and configured - [ ] Python 3.11+ environment setup - [ ] Core dependencies installed (polars, pandas, pyarrow, boto3) - [ ] CloudWatch agent installed and configured - [ ] IAM role attached for cross-account access - [ ] Security group configured with minimal required access - [ ] Directory structure created (/data/temp, /data/processed, etc.) - [ ] Logging configuration implemented - [ ] Health check endpoint available - [ ] System monitoring active - [ ] Automatic security updates configured - [ ] Processing user account created with appropriate permissions **PERFORMANCE TESTING:** 1. CPU benchmark with stress testing 2. Memory allocation testing (allocate 100GB+ arrays) 3. Disk I/O performance testing 4. Network throughput testing to S3 5. Python/Polars performance validation **COST OPTIMIZATION:** - Use Spot Instances for non-critical processing (60-70% cost savings) - Implement auto-shutdown during non-processing hours - Right-size instance based on actual usage patterns - Consider Reserved Instances for long-term usage **DISASTER RECOVERY:** - Automated daily snapshots of EBS volumes - Instance configuration stored in CloudFormation/Terraform - Automated instance replacement procedures - Data backup verification",CloudFlare Data Processing Migration to AWS,High,Infrastructure,aws-ec2,8,,,
Story,Migrate Processing Code to AWS,"**OBJECTIVE:** Migrate and adapt existing CloudFlare firewall log processing code from local Polars/Python environment to AWS-optimized architecture with S3 integration and enhanced performance. **CURRENT CODE ANALYSIS:** **Existing Components (from /Users/adamwitherspoon/projects/ddos/):** - Main processing: `cf_firewall_events.ipynb` (11,647 lines) - Data processor: `ddos_processor.py` - Firewall processor: `firewall_processor.py` - Smart sync: `scripts/smart_sync_ddos_assets.sh` - Processing sources: 2.ford, 3, 47 **CODE MIGRATION STRATEGY:** **Phase 1: Core Processing Logic Migration** 1. Extract processing functions from Jupyter notebooks 2. Modularize code into reusable components 3. Implement S3-native data loading 4. Add AWS-specific error handling and logging **Phase 2: Performance Optimization** 1. Implement parallel processing for multiple sources 2. Add memory management for large datasets 3. Optimize Polars operations for cloud environment 4. Implement streaming processing for large files **DETAILED IMPLEMENTATION:** **New Project Structure:** ``` /opt/cloudflare-processor/ ├── pyproject.toml # UV project configuration ├── src/ │ ├── cloudflare_processor/ │ │ ├── __init__.py │ │ ├── core/ │ │ │ ├── __init__.py │ │ │ ├── data_loader.py # S3-based data loading │ │ │ ├── processor.py # Core Polars processing │ │ │ ├── validator.py # Data validation │ │ │ └── exporter.py # Parquet export │ │ ├── aws/ │ │ │ ├── __init__.py │ │ │ ├── s3_client.py # S3 operations │ │ │ ├── cloudwatch.py # Metrics and logging │ │ │ └── secrets.py # Configuration management │ │ ├── utils/ │ │ │ ├── __init__.py │ │ │ ├── logging.py # Structured logging │ │ │ ├── config.py # Configuration management │ │ │ └── metrics.py # Performance metrics │ │ └── cli/ │ │ ├── __init__.py │ │ └── main.py # Command-line interface ├── tests/ │ ├── unit/ │ ├── integration/ │ └── performance/ ├── config/ │ ├── production.yaml │ ├── development.yaml │ └── logging.yaml └── scripts/ ├── process_daily.sh ├── validate_data.sh └── cleanup.sh ``` **Key Code Adaptations:** **1. S3-Native Data Loading (data_loader.py):** ```python import polars as pl import boto3 from typing import List, Optional import structlog logger = structlog.get_logger() class S3DataLoader: def __init__(self, bucket_name: str, profile_name: Optional[str] = None): self.bucket_name = bucket_name self.s3_client = boto3.client('s3', profile_name=profile_name) def load_firewall_data(self, source: str, date: str, max_files: Optional[int] = None) -> pl.DataFrame: \"\"\"Load CloudFlare firewall data from S3 with optimized streaming.\"\"\" logger.info(f""Loading data for source {source}, date {date}"") # List objects in S3 prefix = f""logs/firewall/di-websites/{source}/{date}/"" objects = self._list_s3_objects(prefix, max_files) # Load data with parallel processing dataframes = [] for obj in objects: df = self._load_single_file(obj) if df is not None: dataframes.append(df) # Combine all dataframes if dataframes: combined_df = pl.concat(dataframes) logger.info(f""Loaded {len(combined_df)} records from {len(dataframes)} files"") return combined_df else: logger.warning(f""No data found for {source}/{date}"") return pl.DataFrame() def _load_single_file(self, s3_key: str) -> Optional[pl.DataFrame]: \"\"\"Load single compressed file from S3.\"\"\" try: # Stream file from S3 response = self.s3_client.get_object(Bucket=self.bucket_name, Key=s3_key) # Handle different compression formats if s3_key.endswith('.gz'): df = pl.read_csv(response['Body'], compression='gzip') elif s3_key.endswith('.parquet'): df = pl.read_parquet(response['Body']) else: df = pl.read_csv(response['Body']) return df except Exception as e: logger.error(f""Failed to load {s3_key}: {e}"") return None ``` **2. Enhanced Processing Engine (processor.py):** ```python import polars as pl from typing import Dict, List import structlog from ..utils.metrics import ProcessingMetrics logger = structlog.get_logger() class FirewallEventProcessor: def __init__(self, config: Dict): self.config = config self.metrics = ProcessingMetrics() def process_firewall_events(self, df: pl.DataFrame, source: str) -> pl.DataFrame: \"\"\"Process firewall events with enhanced analytics.\"\"\" logger.info(f""Processing {len(df)} events for source {source}"") with self.metrics.timer(f""processing_{source}""): # Data cleaning and standardization df_cleaned = self._clean_data(df) # Enhanced analytics df_processed = self._analyze_events(df_cleaned, source) # Add metadata df_final = self._add_metadata(df_processed, source) logger.info(f""Processed to {len(df_final)} records"") return df_final def _clean_data(self, df: pl.DataFrame) -> pl.DataFrame: \"\"\"Clean and standardize firewall event data.\"\"\" return ( df .with_columns([ # Standardize datetime pl.col(""Datetime"").str.strptime(pl.Datetime, format=""%Y-%m-%d %H:%M:%S""), # Clean IP addresses pl.col(""ClientIP"").str.strip(), # Standardize actions pl.col(""Action"").str.to_uppercase(), # Add processing timestamp pl.lit(datetime.now()).alias(""ProcessedAt"") ]) .filter(pl.col(""ClientIP"").is_not_null()) .filter(pl.col(""Action"").is_in([""ALLOW"", ""BLOCK"", ""CHALLENGE""])) ) def _analyze_events(self, df: pl.DataFrame, source: str) -> pl.DataFrame: \"\"\"Perform enhanced analytics on firewall events.\"\"\" return ( df .with_columns([ # Geographic analysis pl.col(""ClientCountry"").fill_null(""Unknown""), # Time-based features pl.col(""Datetime"").dt.hour().alias(""Hour""), pl.col(""Datetime"").dt.day_of_week().alias(""DayOfWeek""), # Risk scoring (placeholder for ML model) pl.when(pl.col(""Action"") == ""BLOCK"") .then(10) .when(pl.col(""Action"") == ""CHALLENGE"") .then(5) .otherwise(1) .alias(""RiskScore""), # Source identification pl.lit(source).alias(""DataSource"") ]) ) ``` **3. AWS Integration (s3_client.py):** ```python import boto3 from botocore.exceptions import ClientError import structlog from typing import Optional logger = structlog.get_logger() class S3Manager: def __init__(self, region_name: str = 'us-east-1'): self.s3_client = boto3.client('s3', region_name=region_name) self.s3_resource = boto3.resource('s3', region_name=region_name) def upload_parquet(self, df: pl.DataFrame, bucket: str, key: str) -> bool: \"\"\"Upload processed DataFrame as parquet to S3.\"\"\" try: # Convert to parquet bytes buffer = BytesIO() df.write_parquet(buffer) buffer.seek(0) # Upload to S3 with metadata self.s3_client.put_object( Bucket=bucket, Key=key, Body=buffer.getvalue(), ContentType='application/octet-stream', Metadata={ 'record_count': str(len(df)), 'processing_date': datetime.now().isoformat(), 'data_source': 'cloudflare_firewall' } ) logger.info(f""Uploaded {len(df)} records to s3://{bucket}/{key}"") return True except ClientError as e: logger.error(f""Failed to upload to S3: {e}"") return False ``` **PERFORMANCE OPTIMIZATIONS:** 1. **Memory Management:** - Streaming data processing for large files - Chunked processing for datasets > available RAM - Garbage collection optimization 2. **Parallel Processing:** - Multi-threaded file loading - Concurrent S3 operations - Process multiple sources simultaneously 3. **Caching Strategy:** - Local SSD caching for frequently accessed data - Intelligent prefetching based on processing patterns **ACCEPTANCE CRITERIA:** - [ ] All existing processing logic migrated from Jupyter notebooks - [ ] S3-native data loading implemented - [ ] Polars processing optimized for cloud environment - [ ] UV project structure created with proper dependencies - [ ] Error handling and logging implemented - [ ] Configuration management system implemented - [ ] CLI interface for processing operations - [ ] Unit tests covering core functionality - [ ] Integration tests with S3 - [ ] Performance benchmarking completed - [ ] Memory usage optimized for EC2 instance - [ ] Parallel processing implemented - [ ] Data validation pipeline integrated - [ ] Metrics and monitoring instrumentation added **TESTING REQUIREMENTS:** 1. **Unit Tests:** - Data loading functions - Processing algorithms - S3 operations - Configuration management 2. **Integration Tests:** - End-to-end processing pipeline - S3 cross-account access - Error handling scenarios 3. **Performance Tests:** - Large dataset processing (1GB+ files) - Memory usage under load - Concurrent processing capabilities **MIGRATION VALIDATION:** - Process sample data from each source (2.ford, 3, 47) - Compare output with existing local processing results - Validate data integrity and completeness - Confirm performance improvements - Test error handling and recovery",CloudFlare Data Processing Migration to AWS,High,Development,code-migration polars,13,,,
Story,Create Processing Automation Scripts,"**OBJECTIVE:** Develop comprehensive automation infrastructure for CloudFlare data processing pipeline including scheduling, monitoring, error handling, and operational maintenance. **AUTOMATION ARCHITECTURE:** **Core Automation Components:** 1. **Daily Processing Orchestrator** - Automated detection of new data 2. **Error Handling & Recovery** - Intelligent retry mechanisms 3. **Data Validation Pipeline** - Automated quality checks 4. **Monitoring & Alerting** - Real-time status monitoring 5. **Maintenance & Cleanup** - Automated housekeeping tasks **DETAILED SCRIPT SPECIFICATIONS:** **1. Daily Processing Orchestrator (process_daily.sh):** ```bash #!/bin/bash # CloudFlare Daily Processing Automation # Location: /opt/cloudflare-processor/scripts/process_daily.sh set -euo pipefail # Configuration SCRIPT_DIR=""$(cd ""$(dirname ""${BASH_SOURCE[0]}"")""  && pwd)"" PROJECT_DIR=""$(dirname ""$SCRIPT_DIR"")"" LOG_DIR=""/data/logs"" CONFIG_FILE=""$PROJECT_DIR/config/production.yaml"" LOCK_FILE=""/tmp/cloudflare_processing.lock"" # Sources to process SOURCES=(""2.ford"" ""3"" ""47"") # S3 Configuration SOURCE_BUCKET=""cloudflare-logs-di"" TARGET_BUCKET=""cars-cloudflare-processed"" S3_PREFIX=""logs/firewall/di-websites"" # Logging setup exec 1> >(tee -a ""$LOG_DIR/daily_processing_$(date +%Y%m%d).log"") exec 2>&1 echo ""[$(date '+%Y-%m-%d %H:%M:%S')] Starting daily CloudFlare processing"" # Function definitions log_info() { echo ""[$(date '+%Y-%m-%d %H:%M:%S')] INFO: $1""; } log_error() { echo ""[$(date '+%Y-%m-%d %H:%M:%S')] ERROR: $1"" >&2; } log_warning() { echo ""[$(date '+%Y-%m-%d %H:%M:%S')] WARNING: $1""; } # Lock file management acquire_lock() { if [ -f ""$LOCK_FILE"" ]; then local pid=$(cat ""$LOCK_FILE"") if kill -0 ""$pid"" 2>/dev/null; then log_error ""Processing already running (PID: $pid)"" exit 1 else log_warning ""Stale lock file found, removing"" rm -f ""$LOCK_FILE"" fi fi echo $$ > ""$LOCK_FILE"" } release_lock() { rm -f ""$LOCK_FILE""; } # Cleanup on exit trap 'release_lock; log_info ""Processing completed""' EXIT # Discover new data to process discover_new_data() { local source=$1 local yesterday=$(date -d ""yesterday"" +%Y%m%d) local today=$(date +%Y%m%d) log_info ""Discovering new data for source $source"" # Check for new data in S3 aws s3 ls ""s3://$SOURCE_BUCKET/$S3_PREFIX/$source/$yesterday/"" --recursive > /tmp/available_files_$source.txt 2>/dev/null || true aws s3 ls ""s3://$SOURCE_BUCKET/$S3_PREFIX/$source/$today/"" --recursive >> /tmp/available_files_$source.txt 2>/dev/null || true # Check what's already processed local processed_files=""/data/processed/$source/"" if [ -d ""$processed_files"" ]; then ls ""$processed_files""*.parquet 2>/dev/null | sed 's/.*_([0-9]{8}).parquet/\1/' > /tmp/processed_dates_$source.txt || touch /tmp/processed_dates_$source.txt else touch /tmp/processed_dates_$source.txt fi # Determine what needs processing comm -23 <(grep -o '[0-9]\{8\}' /tmp/available_files_$source.txt | sort -u) <(sort /tmp/processed_dates_$source.txt) > /tmp/pending_dates_$source.txt log_info ""Found $(wc -l < /tmp/pending_dates_$source.txt) dates to process for $source"" } # Process single date for source process_date() { local source=$1 local date=$2 log_info ""Processing $source for date $date"" # Create processing metrics local start_time=$(date +%s) local temp_dir=""/data/temp/$source/$date"" local output_dir=""/data/processed/$source"" # Ensure directories exist mkdir -p ""$temp_dir"" ""$output_dir"" # Download data from S3 log_info ""Downloading data for $source/$date"" if ! aws s3 sync ""s3://$SOURCE_BUCKET/$S3_PREFIX/$source/$date/"" ""$temp_dir/"" --quiet; then log_error ""Failed to download data for $source/$date"" return 1 fi # Process with Python local record_count=0 if cd ""$PROJECT_DIR"" && uv run python -m cloudflare_processor.cli.main process \ --source ""$source"" \ --date ""$date"" \ --input-dir ""$temp_dir"" \ --output-dir ""$output_dir"" \ --config ""$CONFIG_FILE""; then # Get processing results record_count=$(uv run python -c ""import polars as pl; df = pl.read_parquet('$output_dir/firewall_logs_${source}_${date}.parquet'); print(len(df))"") log_info ""Successfully processed $record_count records for $source/$date"" # Upload to S3 aws s3 cp ""$output_dir/firewall_logs_${source}_${date}.parquet"" ""s3://$TARGET_BUCKET/processed/$source/"" aws s3 cp ""$output_dir/firewall_logs_${source}_${date}.metadata.json"" ""s3://$TARGET_BUCKET/processed/$source/"" # Cleanup temp data rm -rf ""$temp_dir"" # Send success metrics local end_time=$(date +%s) local duration=$((end_time - start_time)) aws cloudwatch put-metric-data \ --namespace ""CloudFlare/Processing"" \ --metric-data MetricName=ProcessingDuration,Value=$duration,Unit=Seconds,Dimensions=Source=$source MetricName=RecordsProcessed,Value=$record_count,Unit=Count,Dimensions=Source=$source else log_error ""Processing failed for $source/$date"" return 1 fi } # Main processing loop main() { acquire_lock log_info ""Starting daily processing for $(date +%Y-%m-%d)"" local total_processed=0 local total_errors=0 # Process each source for source in ""${SOURCES[@]}""; do log_info ""Processing source: $source"" discover_new_data ""$source"" # Process each pending date while IFS= read -r date; do if [ -n ""$date"" ]; then if process_date ""$source"" ""$date""; then ((total_processed++)) else ((total_errors++)) fi fi done < ""/tmp/pending_dates_$source.txt"" done # Send summary metrics aws cloudwatch put-metric-data \ --namespace ""CloudFlare/Processing"" \ --metric-data MetricName=DailyProcessedCount,Value=$total_processed,Unit=Count MetricName=DailyErrorCount,Value=$total_errors,Unit=Count log_info ""Daily processing complete: $total_processed processed, $total_errors errors"" # Trigger data validation if [ $total_processed -gt 0 ]; then ""$SCRIPT_DIR/validate_data.sh"" fi } # Execute main function main ""$@"" ``` **2. Error Handling & Recovery (error_recovery.sh):** ```bash #!/bin/bash # CloudFlare Processing Error Recovery # Handles failed processing jobs and implements retry logic set -euo pipefail LOG_DIR=""/data/logs"" ERROR_LOG=""$LOG_DIR/errors_$(date +%Y%m%d).log"" MAX_RETRIES=3 RETRY_DELAY=300 # 5 minutes # Function to retry failed processing retry_failed_processing() { local source=$1 local date=$2 local attempt=$3 log_info ""Retry attempt $attempt for $source/$date"" if [ $attempt -le $MAX_RETRIES ]; then sleep $RETRY_DELAY if process_date ""$source"" ""$date""; then log_info ""Retry successful for $source/$date"" aws sns publish \ --topic-arn ""arn:aws:sns:us-east-1:813388701013:cloudflare-processing-alerts"" \ --message ""Processing recovered for $source/$date after $attempt attempts"" else log_error ""Retry $attempt failed for $source/$date"" retry_failed_processing ""$source"" ""$date"" $((attempt + 1)) fi else log_error ""Max retries exceeded for $source/$date"" # Send critical alert aws sns publish \ --topic-arn ""arn:aws:sns:us-east-1:813388701013:cloudflare-processing-critical"" \ --message ""CRITICAL: Processing failed for $source/$date after $MAX_RETRIES attempts"" fi } ``` **3. Data Validation Pipeline (validate_data.sh):** ```bash #!/bin/bash # CloudFlare Data Validation Pipeline # Validates processed data integrity and quality set -euo pipefail PROJECT_DIR=""/opt/cloudflare-processor"" LOG_DIR=""/data/logs"" VALIDATION_LOG=""$LOG_DIR/validation_$(date +%Y%m%d).log"" # Validation functions validate_record_counts() { local source=$1 local date=$2 log_info ""Validating record counts for $source/$date"" # Get source record count (from raw files) local source_count=$(aws s3 ls ""s3://cloudflare-logs-di/logs/firewall/di-websites/$source/$date/"" --recursive | awk '{sum += $3} END {print sum}') # Get processed record count local processed_file=""/data/processed/$source/firewall_logs_${source}_${date}.parquet"" if [ -f ""$processed_file"" ]; then local processed_count=$(cd ""$PROJECT_DIR"" && uv run python -c ""import polars as pl; df = pl.read_parquet('$processed_file'); print(len(df))"") # Allow for 1% variance in record counts (due to data cleaning) local variance=$(echo ""scale=2; $processed_count / $source_count"" | bc) if (( $(echo ""$variance >= 0.99 && $variance <= 1.01"" | bc -l) )); then log_info ""Record count validation passed: $processed_count records"" return 0 else log_error ""Record count validation failed: source=$source_count, processed=$processed_count"" return 1 fi else log_error ""Processed file not found: $processed_file"" return 1 fi } validate_schema() { local processed_file=$1 log_info ""Validating schema for $processed_file"" cd ""$PROJECT_DIR"" && uv run python -c "" import polars as pl import sys df = pl.read_parquet('$processed_file') required_columns = ['Datetime', 'ClientIP', 'Action', 'ClientCountry', 'DataSource'] missing_columns = [col for col in required_columns if col not in df.columns] if missing_columns: print(f'Missing columns: {missing_columns}') sys.exit(1) else: print('Schema validation passed') "" } ``` **4. Monitoring & Health Check (health_check.sh):** ```bash #!/bin/bash # CloudFlare Processing Health Check # Monitors system health and processing status set -euo pipefail check_system_resources() { # Check disk space local disk_usage=$(df /data | awk 'NR==2 {print $5}' | sed 's/%//') if [ $disk_usage -gt 90 ]; then aws sns publish \ --topic-arn ""arn:aws:sns:us-east-1:813388701013:cloudflare-processing-alerts"" \ --message ""WARNING: Disk usage at ${disk_usage}% on processing instance"" fi # Check memory usage local mem_usage=$(free | awk 'NR==2{printf ""%.0f"", $3*100/$2}') if [ $mem_usage -gt 85 ]; then aws sns publish \ --topic-arn ""arn:aws:sns:us-east-1:813388701013:cloudflare-processing-alerts"" \ --message ""WARNING: Memory usage at ${mem_usage}% on processing instance"" fi } check_processing_status() { # Check if processing is running local processing_pid=$(pgrep -f ""cloudflare_processor"" || echo """") if [ -n ""$processing_pid"" ]; then echo ""Processing active (PID: $processing_pid)"" else echo ""No active processing"" fi # Check last successful processing local last_success=$(find /data/processed -name ""*.parquet"" -printf '%T@ %p\n' | sort -n | tail -1 | cut -d' ' -f1) local current_time=$(date +%s) local time_diff=$((current_time - ${last_success:-0})) if [ $time_diff -gt 86400 ]; then # 24 hours aws sns publish \ --topic-arn ""arn:aws:sns:us-east-1:813388701013:cloudflare-processing-alerts"" \ --message ""WARNING: No successful processing in last 24 hours"" fi } ``` **5. Cleanup & Maintenance (cleanup.sh):** ```bash #!/bin/bash # CloudFlare Processing Cleanup # Automated maintenance and cleanup tasks set -euo pipefail LOG_RETENTION_DAYS=30 TEMP_DATA_RETENTION_HOURS=24 cleanup_logs() { log_info ""Cleaning up old log files"" find /data/logs -name ""*.log"" -mtime +$LOG_RETENTION_DAYS -delete } cleanup_temp_data() { log_info ""Cleaning up temporary data"" find /data/temp -type f -mmin +$((TEMP_DATA_RETENTION_HOURS * 60)) -delete find /data/temp -type d -empty -delete } optimize_storage() { log_info ""Optimizing storage"" # Compress old processed files find /data/processed -name ""*.parquet"" -mtime +7 -exec gzip {} \; # Clean up package cache uv cache clean } ``` **SCHEDULING CONFIGURATION:** **Crontab Setup:** ``` # CloudFlare Processing Automation # Daily processing at 2 AM 0 2 * * * /opt/cloudflare-processor/scripts/process_daily.sh # Health checks every 15 minutes */15 * * * * /opt/cloudflare-processor/scripts/health_check.sh # Cleanup weekly on Sunday at 3 AM 0 3 * * 0 /opt/cloudflare-processor/scripts/cleanup.sh # Validation runs after daily processing 30 2 * * * /opt/cloudflare-processor/scripts/validate_data.sh ``` **ACCEPTANCE CRITERIA:** - [ ] Daily processing orchestrator implemented with source discovery - [ ] Error handling and retry logic with exponential backoff - [ ] Data validation pipeline with integrity checks - [ ] Health monitoring with system resource checks - [ ] Automated cleanup and maintenance scripts - [ ] Comprehensive logging with structured format - [ ] CloudWatch metrics integration - [ ] SNS alerting for failures and warnings - [ ] Lock file management to prevent concurrent runs - [ ] Configuration management for different environments - [ ] Cron scheduling configured and tested - [ ] Documentation for operational procedures **MONITORING INTEGRATION:** - CloudWatch custom metrics for processing duration, record counts, error rates - SNS topics for different alert levels (info, warning, critical) - CloudWatch Logs integration for centralized log management - Dashboard creation for operational visibility **ERROR SCENARIOS HANDLED:** 1. S3 access failures (network, permissions) 2. Processing failures (memory, computation errors) 3. Data validation failures 4. System resource exhaustion 5. Concurrent processing attempts 6. Configuration errors 7. AWS service outages **TESTING REQUIREMENTS:** - Unit tests for each script function - Integration tests with AWS services - Failure scenario testing - Performance testing under load - Cron schedule validation - Alert testing for all scenarios",CloudFlare Data Processing Migration to AWS,Medium,Development,automation,8,,,
Story,Setup CloudWatch Monitoring,"**OBJECTIVE:** Implement comprehensive CloudWatch monitoring infrastructure for CloudFlare processing pipeline with custom metrics, dashboards, alarms, and automated alerting to ensure operational excellence and proactive issue detection. **MONITORING ARCHITECTURE:** **Monitoring Layers:** 1. **Infrastructure Monitoring** - EC2 instance metrics 2. **Application Performance Monitoring** - Processing pipeline metrics 3. **Business Metrics** - Data processing KPIs 4. **Security & Compliance** - Access and audit metrics 5. **Cost Monitoring** - Resource utilization and spend **DETAILED IMPLEMENTATION:** **1. CloudWatch Agent Configuration:** ```json { ""agent"": { ""metrics_collection_interval"": 60, ""run_as_user"": ""cwagent"" }, ""metrics"": { ""namespace"": ""CloudFlare/Processing"", ""metrics_collected"": { ""cpu"": { ""measurement"": [ ""cpu_usage_idle"", ""cpu_usage_iowait"", ""cpu_usage_user"", ""cpu_usage_system"", ""cpu_usage_steal"", ""cpu_usage_nice"", ""cpu_usage_softirq"", ""cpu_usage_irq"" ], ""metrics_collection_interval"": 60, ""resources"": [""*""], ""totalcpu"": false }, ""disk"": { ""measurement"": [ ""used_percent"", ""inodes_free"" ], ""metrics_collection_interval"": 60, ""resources"": [""*""] }, ""diskio"": { ""measurement"": [ ""io_time"", ""read_bytes"", ""write_bytes"", ""reads"", ""writes"" ], ""metrics_collection_interval"": 60, ""resources"": [""*""] }, ""mem"": { ""measurement"": [""mem_used_percent""], ""metrics_collection_interval"": 60 }, ""netstat"": { ""measurement"": [ ""tcp_established"", ""tcp_time_wait"" ], ""metrics_collection_interval"": 60 }, ""processes"": { ""measurement"": [ ""running"", ""sleeping"", ""dead"" ] } } }, ""logs"": { ""logs_collected"": { ""files"": { ""collect_list"": [ { ""file_path"": ""/data/logs/processing.log"", ""log_group_name"": ""cloudflare-processing"", ""log_stream_name"": ""{instance_id}/processing"", ""timezone"": ""UTC"" }, { ""file_path"": ""/data/logs/errors.log"", ""log_group_name"": ""cloudflare-processing-errors"", ""log_stream_name"": ""{instance_id}/errors"", ""timezone"": ""UTC"" }, { ""file_path"": ""/data/logs/validation.log"", ""log_group_name"": ""cloudflare-validation"", ""log_stream_name"": ""{instance_id}/validation"", ""timezone"": ""UTC"" } ] } } } } ``` **2. Custom Application Metrics:** ```python # Custom CloudWatch Metrics Integration import boto3 import time from typing import Dict, Any import structlog logger = structlog.get_logger() class CloudWatchMetrics: def __init__(self, namespace: str = ""CloudFlare/Processing""): self.cloudwatch = boto3.client('cloudwatch') self.namespace = namespace def put_processing_metrics(self, source: str, metrics: Dict[str, Any]): \"\"\"Send processing metrics to CloudWatch.\"\"\" metric_data = [] # Processing duration if 'duration' in metrics: metric_data.append({ 'MetricName': 'ProcessingDuration', 'Value': metrics['duration'], 'Unit': 'Seconds', 'Dimensions': [{'Name': 'Source', 'Value': source}] }) # Records processed if 'records_processed' in metrics: metric_data.append({ 'MetricName': 'RecordsProcessed', 'Value': metrics['records_processed'], 'Unit': 'Count', 'Dimensions': [{'Name': 'Source', 'Value': source}] }) # Processing rate (records per second) if 'processing_rate' in metrics: metric_data.append({ 'MetricName': 'ProcessingRate', 'Value': metrics['processing_rate'], 'Unit': 'Count/Second', 'Dimensions': [{'Name': 'Source', 'Value': source}] }) # Memory usage during processing if 'memory_usage_mb' in metrics: metric_data.append({ 'MetricName': 'ProcessingMemoryUsage', 'Value': metrics['memory_usage_mb'], 'Unit': 'Megabytes', 'Dimensions': [{'Name': 'Source', 'Value': source}] }) # Data quality score if 'quality_score' in metrics: metric_data.append({ 'MetricName': 'DataQualityScore', 'Value': metrics['quality_score'], 'Unit': 'Percent', 'Dimensions': [{'Name': 'Source', 'Value': source}] }) # Error count if 'error_count' in metrics: metric_data.append({ 'MetricName': 'ProcessingErrors', 'Value': metrics['error_count'], 'Unit': 'Count', 'Dimensions': [{'Name': 'Source', 'Value': source}] }) # Send metrics to CloudWatch try: self.cloudwatch.put_metric_data( Namespace=self.namespace, MetricData=metric_data ) logger.info(f""Sent {len(metric_data)} metrics to CloudWatch for source {source}"") except Exception as e: logger.error(f""Failed to send metrics to CloudWatch: {e}"") def put_s3_transfer_metrics(self, operation: str, metrics: Dict[str, Any]): \"\"\"Send S3 transfer metrics.\"\"\" metric_data = [] if 'transfer_size_mb' in metrics: metric_data.append({ 'MetricName': 'S3TransferSize', 'Value': metrics['transfer_size_mb'], 'Unit': 'Megabytes', 'Dimensions': [{'Name': 'Operation', 'Value': operation}] }) if 'transfer_duration' in metrics: metric_data.append({ 'MetricName': 'S3TransferDuration', 'Value': metrics['transfer_duration'], 'Unit': 'Seconds', 'Dimensions': [{'Name': 'Operation', 'Value': operation}] }) if 'transfer_rate_mbps' in metrics: metric_data.append({ 'MetricName': 'S3TransferRate', 'Value': metrics['transfer_rate_mbps'], 'Unit': 'Megabytes/Second', 'Dimensions': [{'Name': 'Operation', 'Value': operation}] }) try: self.cloudwatch.put_metric_data( Namespace=self.namespace, MetricData=metric_data ) except Exception as e: logger.error(f""Failed to send S3 metrics: {e}"") ``` **3. CloudWatch Alarms Configuration:** ```yaml # CloudWatch Alarms (CloudFormation/Terraform format) Resources: # High CPU Usage Alarm HighCPUAlarm: Type: AWS::CloudWatch::Alarm Properties: AlarmName: CloudFlare-Processing-HighCPU AlarmDescription: CPU usage is above 80% for 10 minutes MetricName: CPUUtilization Namespace: AWS/EC2 Statistic: Average Period: 300 EvaluationPeriods: 2 Threshold: 80 ComparisonOperator: GreaterThanThreshold Dimensions: - Name: InstanceId Value: !Ref ProcessingInstance AlarmActions: - !Ref ProcessingAlertsTopicArn # High Memory Usage Alarm HighMemoryAlarm: Type: AWS::CloudWatch::Alarm Properties: AlarmName: CloudFlare-Processing-HighMemory AlarmDescription: Memory usage is above 85% for 5 minutes MetricName: mem_used_percent Namespace: CloudFlare/Processing Statistic: Average Period: 300 EvaluationPeriods: 1 Threshold: 85 ComparisonOperator: GreaterThanThreshold AlarmActions: - !Ref ProcessingAlertsTopicArn # Disk Space Alarm HighDiskUsageAlarm: Type: AWS::CloudWatch::Alarm Properties: AlarmName: CloudFlare-Processing-HighDisk AlarmDescription: Disk usage is above 90% MetricName: used_percent Namespace: CloudFlare/Processing Statistic: Average Period: 300 EvaluationPeriods: 1 Threshold: 90 ComparisonOperator: GreaterThanThreshold AlarmActions: - !Ref ProcessingCriticalTopicArn # Processing Failure Alarm ProcessingFailureAlarm: Type: AWS::CloudWatch::Alarm Properties: AlarmName: CloudFlare-Processing-Failures AlarmDescription: Processing errors detected MetricName: ProcessingErrors Namespace: CloudFlare/Processing Statistic: Sum Period: 300 EvaluationPeriods: 1 Threshold: 1 ComparisonOperator: GreaterThanOrEqualToThreshold AlarmActions: - !Ref ProcessingAlertsTopicArn # No Recent Processing Alarm NoRecentProcessingAlarm: Type: AWS::CloudWatch::Alarm Properties: AlarmName: CloudFlare-Processing-NoActivity AlarmDescription: No processing activity in last 25 hours MetricName: RecordsProcessed Namespace: CloudFlare/Processing Statistic: Sum Period: 3600 EvaluationPeriods: 25 Threshold: 1 ComparisonOperator: LessThanThreshold TreatMissingData: breaching AlarmActions: - !Ref ProcessingAlertsTopicArn # S3 Transfer Rate Alarm SlowS3TransferAlarm: Type: AWS::CloudWatch::Alarm Properties: AlarmName: CloudFlare-Processing-SlowS3Transfer AlarmDescription: S3 transfer rate is below expected threshold MetricName: S3TransferRate Namespace: CloudFlare/Processing Statistic: Average Period: 300 EvaluationPeriods: 2 Threshold: 10 ComparisonOperator: LessThanThreshold AlarmActions: - !Ref ProcessingAlertsTopicArn ``` **4. CloudWatch Dashboard:** ```json { ""widgets"": [ { ""type"": ""metric"", ""x"": 0, ""y"": 0, ""width"": 12, ""height"": 6, ""properties"": { ""metrics"": [ [ ""CloudFlare/Processing"", ""RecordsProcessed"", ""Source"", ""2.ford"" ], [ ""..."", ""3"" ], [ ""..."", ""47"" ] ], ""view"": ""timeSeries"", ""stacked"": false, ""region"": ""us-east-1"", ""title"": ""Records Processed by Source"", ""period"": 300, ""stat"": ""Sum"" } }, { ""type"": ""metric"", ""x"": 12, ""y"": 0, ""width"": 12, ""height"": 6, ""properties"": { ""metrics"": [ [ ""CloudFlare/Processing"", ""ProcessingDuration"", ""Source"", ""2.ford"" ], [ ""..."", ""3"" ], [ ""..."", ""47"" ] ], ""view"": ""timeSeries"", ""stacked"": false, ""region"": ""us-east-1"", ""title"": ""Processing Duration by Source"", ""period"": 300, ""stat"": ""Average"" } }, { ""type"": ""metric"", ""x"": 0, ""y"": 6, ""width"": 8, ""height"": 6, ""properties"": { ""metrics"": [ [ ""AWS/EC2"", ""CPUUtilization"", ""InstanceId"", ""${InstanceId}"" ] ], ""view"": ""timeSeries"", ""stacked"": false, ""region"": ""us-east-1"", ""title"": ""EC2 CPU Utilization"", ""period"": 300, ""stat"": ""Average"" } }, { ""type"": ""metric"", ""x"": 8, ""y"": 6, ""width"": 8, ""height"": 6, ""properties"": { ""metrics"": [ [ ""CloudFlare/Processing"", ""mem_used_percent"" ] ], ""view"": ""timeSeries"", ""stacked"": false, ""region"": ""us-east-1"", ""title"": ""Memory Usage"", ""period"": 300, ""stat"": ""Average"" } }, { ""type"": ""metric"", ""x"": 16, ""y"": 6, ""width"": 8, ""height"": 6, ""properties"": { ""metrics"": [ [ ""CloudFlare/Processing"", ""used_percent"", ""device"", ""/dev/xvda1"" ], [ ""..."", ""/dev/xvdf"" ] ], ""view"": ""timeSeries"", ""stacked"": false, ""region"": ""us-east-1"", ""title"": ""Disk Usage"", ""period"": 300, ""stat"": ""Average"" } }, { ""type"": ""log"", ""x"": 0, ""y"": 12, ""width"": 24, ""height"": 6, ""properties"": { ""query"": ""SOURCE 'cloudflare-processing-errors' | fields @timestamp, @message | sort @timestamp desc | limit 100"", ""region"": ""us-east-1"", ""title"": ""Recent Processing Errors"", ""view"": ""table"" } } ] } ``` **5. SNS Topics and Subscriptions:** ```yaml # SNS Topics for Alerting ProcessingAlertsTopicArn: Type: AWS::SNS::Topic Properties: TopicName: cloudflare-processing-alerts DisplayName: CloudFlare Processing Alerts Subscription: - Protocol: email Endpoint: devops-team@company.com - Protocol: slack Endpoint: !Ref SlackWebhookURL ProcessingCriticalTopicArn: Type: AWS::SNS::Topic Properties: TopicName: cloudflare-processing-critical DisplayName: CloudFlare Processing Critical Alerts Subscription: - Protocol: email Endpoint: oncall-team@company.com - Protocol: sms Endpoint: +1234567890 ``` **6. Log Insights Queries:** ```sql # Processing Performance Analysis fields @timestamp, source, duration, records_processed | filter @message like /Processing complete/ | stats avg(duration), sum(records_processed) by source | sort avg(duration) desc # Error Analysis fields @timestamp, @message | filter @message like /ERROR/ | stats count() by bin(5m) | sort @timestamp desc # Data Quality Monitoring fields @timestamp, source, quality_score | filter quality_score < 95 | sort @timestamp desc ``` **ACCEPTANCE CRITERIA:** - [ ] CloudWatch Agent installed and configured on EC2 instance - [ ] Custom application metrics implemented and tested - [ ] Infrastructure monitoring (CPU, memory, disk, network) active - [ ] Application performance metrics (processing duration, throughput) tracked - [ ] Business metrics (records processed, data quality) monitored - [ ] CloudWatch alarms configured for all critical thresholds - [ ] SNS topics created with appropriate subscriptions - [ ] CloudWatch dashboard created with key operational metrics - [ ] Log aggregation configured for all application logs - [ ] Log Insights queries created for troubleshooting - [ ] Alarm testing completed for all scenarios - [ ] Documentation created for monitoring procedures - [ ] Runbook created for alarm response procedures **MONITORING METRICS SUMMARY:** **Infrastructure Metrics:** - CPU Utilization (target: <80%) - Memory Usage (target: <85%) - Disk Usage (target: <90%) - Network I/O - EBS Volume Performance **Application Metrics:** - Processing Duration per Source - Records Processed per Hour - Processing Rate (records/second) - Error Count and Rate - Data Quality Score - S3 Transfer Performance **Business Metrics:** - Daily Processing Volume - Processing Success Rate - Data Freshness (time since last processing) - Cost per Record Processed **ALERTING STRATEGY:** **Warning Level (Email):** - CPU > 80% for 10 minutes - Memory > 85% for 5 minutes - Disk > 90% - Processing errors detected - Slow S3 transfers **Critical Level (Email + SMS):** - CPU > 95% for 5 minutes - Memory > 95% for 2 minutes - Disk > 95% - No processing activity for 25 hours - Processing failure rate > 10%",CloudFlare Data Processing Migration to AWS,Medium,Infrastructure,aws-cloudwatch monitoring,5,,,
Story,Implement Data Validation Pipeline,"**OBJECTIVE:** Create comprehensive data validation pipeline to ensure 100% data integrity during CloudFlare log migration and ongoing processing, with automated quality checks, anomaly detection, and detailed reporting. **VALIDATION ARCHITECTURE:** **Validation Layers:** 1. **Schema Validation** - Column presence and data types 2. **Data Quality Validation** - Completeness, accuracy, consistency 3. **Business Logic Validation** - Domain-specific rules and constraints 4. **Statistical Validation** - Anomaly detection and trend analysis 5. **Cross-Reference Validation** - Comparison with source data **DETAILED IMPLEMENTATION:** **1. Core Validation Framework (validator.py):** ```python import polars as pl import boto3 from typing import Dict, List, Tuple, Optional, Any from dataclasses import dataclass from enum import Enum import structlog import json from datetime import datetime, timedelta logger = structlog.get_logger() class ValidationSeverity(Enum): INFO = ""info"" WARNING = ""warning"" ERROR = ""error"" CRITICAL = ""critical"" @dataclass class ValidationResult: check_name: str severity: ValidationSeverity status: bool message: str details: Dict[str, Any] timestamp: datetime record_count: Optional[int] = None class CloudFlareDataValidator: def __init__(self, config: Dict[str, Any]): self.config = config self.cloudwatch = boto3.client('cloudwatch') self.s3_client = boto3.client('s3') self.validation_results: List[ValidationResult] = [] def validate_processed_data(self, df: pl.DataFrame, source: str, date: str, source_metadata: Dict = None) -> Dict[str, Any]: \"\"\"Comprehensive validation of processed CloudFlare data.\"\"\" logger.info(f""Starting validation for {source}/{date} with {len(df)} records"") self.validation_results = [] # Schema validation self._validate_schema(df, source) # Data quality validation self._validate_data_quality(df, source) # Business logic validation self._validate_business_logic(df, source) # Statistical validation self._validate_statistics(df, source, date) # Cross-reference validation if source_metadata: self._validate_cross_reference(df, source_metadata, source, date) # Generate validation report report = self._generate_validation_report(source, date) # Send metrics to CloudWatch self._send_validation_metrics(source, report) return report def _validate_schema(self, df: pl.DataFrame, source: str): \"\"\"Validate DataFrame schema against expected structure.\"\"\" expected_schema = { 'Datetime': pl.Datetime, 'ClientIP': pl.Utf8, 'ClientRequestHost': pl.Utf8, 'Action': pl.Utf8, 'ClientCountry': pl.Utf8, 'Source': pl.Utf8, 'RiskScore': pl.Int64, 'ProcessedAt': pl.Datetime } # Check required columns missing_columns = [] for col_name, expected_type in expected_schema.items(): if col_name not in df.columns: missing_columns.append(col_name) self.validation_results.append(ValidationResult( check_name=""missing_columns"", severity=ValidationSeverity.CRITICAL, status=len(missing_columns) == 0, message=f""Missing required columns: {missing_columns}"" if missing_columns else ""All required columns present"", details={""missing_columns"": missing_columns}, timestamp=datetime.now(), record_count=len(df) )) # Check data types for col_name, expected_type in expected_schema.items(): if col_name in df.columns: actual_type = df[col_name].dtype type_match = actual_type == expected_type self.validation_results.append(ValidationResult( check_name=f""data_type_{col_name}"", severity=ValidationSeverity.ERROR if not type_match else ValidationSeverity.INFO, status=type_match, message=f""Column {col_name}: expected {expected_type}, got {actual_type}"", details={""column"": col_name, ""expected"": str(expected_type), ""actual"": str(actual_type)}, timestamp=datetime.now(), record_count=len(df) )) def _validate_data_quality(self, df: pl.DataFrame, source: str): \"\"\"Validate data quality metrics.\"\"\" total_records = len(df) # Null value checks for column in df.columns: null_count = df[column].null_count() null_percentage = (null_count / total_records) * 100 if null_percentage > 0 # Allow some null values for certain columns acceptable_null_columns = ['ClientCountry', 'ClientRequestHost'] severity = ValidationSeverity.WARNING if column in acceptable_null_columns else ValidationSeverity.ERROR threshold = 10.0 if column in acceptable_null_columns else 1.0 self.validation_results.append(ValidationResult( check_name=f""null_values_{column}"", severity=severity if null_percentage > threshold else ValidationSeverity.INFO, status=null_percentage <= threshold, message=f""Column {column} has {null_percentage:.2f}% null values"", details={""column"": column, ""null_count"": null_count, ""null_percentage"": null_percentage}, timestamp=datetime.now(), record_count=total_records )) # Duplicate record check if 'ClientIP' in df.columns and 'Datetime' in df.columns: duplicate_count = df.group_by(['ClientIP', 'Datetime']).count().filter(pl.col('count') > 1).height duplicate_percentage = (duplicate_count / total_records) * 100 self.validation_results.append(ValidationResult( check_name=""duplicate_records"", severity=ValidationSeverity.WARNING if duplicate_percentage > 1.0 else ValidationSeverity.INFO, status=duplicate_percentage <= 1.0, message=f""{duplicate_count} duplicate records found ({duplicate_percentage:.2f}%)"", details={""duplicate_count"": duplicate_count, ""duplicate_percentage"": duplicate_percentage}, timestamp=datetime.now(), record_count=total_records )) # Data range validation if 'Datetime' in df.columns: date_range = df['Datetime'].min(), df['Datetime'].max() expected_date = datetime.strptime(date, '%Y%m%d').date() date_validation = all( expected_date <= dt.date() <= expected_date + timedelta(days=1) for dt in date_range if dt is not None ) self.validation_results.append(ValidationResult( check_name=""date_range"", severity=ValidationSeverity.ERROR if not date_validation else ValidationSeverity.INFO, status=date_validation, message=f""Date range validation: {date_range[0]} to {date_range[1]}"", details={""min_date"": str(date_range[0]), ""max_date"": str(date_range[1]), ""expected_date"": date}, timestamp=datetime.now(), record_count=total_records )) def _validate_business_logic(self, df: pl.DataFrame, source: str): \"\"\"Validate business logic and domain constraints.\"\"\" total_records = len(df) # Valid IP address format (basic check) if 'ClientIP' in df.columns: invalid_ip_count = df.filter( ~pl.col('ClientIP').str.contains(r'^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$') ).height invalid_ip_percentage = (invalid_ip_count / total_records) * 100 self.validation_results.append(ValidationResult( check_name=""ip_address_format"", severity=ValidationSeverity.ERROR if invalid_ip_percentage > 5.0 else ValidationSeverity.WARNING, status=invalid_ip_percentage <= 5.0, message=f""{invalid_ip_count} records with invalid IP format ({invalid_ip_percentage:.2f}%)"", details={""invalid_ip_count"": invalid_ip_count, ""invalid_ip_percentage"": invalid_ip_percentage}, timestamp=datetime.now(), record_count=total_records )) # Valid action values if 'Action' in df.columns: valid_actions = ['ALLOW', 'BLOCK', 'CHALLENGE', 'LOG'] invalid_action_count = df.filter( ~pl.col('Action').is_in(valid_actions) ).height invalid_action_percentage = (invalid_action_count / total_records) * 100 self.validation_results.append(ValidationResult( check_name=""valid_actions"", severity=ValidationSeverity.ERROR if invalid_action_percentage > 0 else ValidationSeverity.INFO, status=invalid_action_percentage == 0, message=f""{invalid_action_count} records with invalid actions ({invalid_action_percentage:.2f}%)"", details={""invalid_action_count"": invalid_action_count, ""valid_actions"": valid_actions}, timestamp=datetime.now(), record_count=total_records )) # Risk score validation if 'RiskScore' in df.columns: invalid_risk_count = df.filter( (pl.col('RiskScore') < 0) | (pl.col('RiskScore') > 100) ).height invalid_risk_percentage = (invalid_risk_count / total_records) * 100 self.validation_results.append(ValidationResult( check_name=""risk_score_range"", severity=ValidationSeverity.ERROR if invalid_risk_percentage > 0 else ValidationSeverity.INFO, status=invalid_risk_percentage == 0, message=f""{invalid_risk_count} records with invalid risk scores ({invalid_risk_percentage:.2f}%)"", details={""invalid_risk_count"": invalid_risk_count, ""expected_range"": ""0-100""}, timestamp=datetime.now(), record_count=total_records )) def _validate_statistics(self, df: pl.DataFrame, source: str, date: str): \"\"\"Statistical validation and anomaly detection.\"\"\" total_records = len(df) # Historical comparison (if available) historical_avg = self._get_historical_average(source) if historical_avg: deviation = abs(total_records - historical_avg) / historical_avg * 100 anomaly_threshold = 50.0 # 50% deviation threshold self.validation_results.append(ValidationResult( check_name=""record_count_anomaly"", severity=ValidationSeverity.WARNING if deviation > anomaly_threshold else ValidationSeverity.INFO, status=deviation <= anomaly_threshold, message=f""Record count deviation: {deviation:.1f}% from historical average"", details={ ""current_count"": total_records, ""historical_average"": historical_avg, ""deviation_percentage"": deviation }, timestamp=datetime.now(), record_count=total_records )) # Action distribution analysis if 'Action' in df.columns: action_distribution = df['Action'].value_counts().to_dict() total_actions = sum(action_distribution.values()) block_percentage = (action_distribution.get('BLOCK', 0) / total_actions) * 100 # Alert if block rate is unusually high high_block_threshold = 20.0 self.validation_results.append(ValidationResult( check_name=""block_rate_analysis"", severity=ValidationSeverity.WARNING if block_percentage > high_block_threshold else ValidationSeverity.INFO, status=block_percentage <= high_block_threshold, message=f""Block rate: {block_percentage:.1f}% of total actions"", details={""action_distribution"": action_distribution, ""block_percentage"": block_percentage}, timestamp=datetime.now(), record_count=total_records )) def _validate_cross_reference(self, df: pl.DataFrame, source_metadata: Dict, source: str, date: str): \"\"\"Cross-reference validation with source data.\"\"\" processed_count = len(df) source_count = source_metadata.get('record_count', 0) # Record count comparison variance_threshold = 5.0 # Allow 5% variance if source_count > 0: variance = abs(processed_count - source_count) / source_count * 100 else: variance = 100.0 if processed_count > 0 else 0.0 self.validation_results.append(ValidationResult( check_name=""source_record_count_match"", severity=ValidationSeverity.ERROR if variance > variance_threshold else ValidationSeverity.INFO, status=variance <= variance_threshold, message=f""Record count variance: {variance:.1f}% (processed: {processed_count}, source: {source_count})"", details={ ""processed_count"": processed_count, ""source_count"": source_count, ""variance_percentage"": variance }, timestamp=datetime.now(), record_count=processed_count )) def _generate_validation_report(self, source: str, date: str) -> Dict[str, Any]: \"\"\"Generate comprehensive validation report.\"\"\" total_checks = len(self.validation_results) passed_checks = sum(1 for result in self.validation_results if result.status) failed_checks = total_checks - passed_checks # Categorize by severity severity_counts = {} for severity in ValidationSeverity: severity_counts[severity.value] = sum( 1 for result in self.validation_results if result.severity == severity ) # Overall validation status overall_status = ""PASSED"" if failed_checks == 0 else ""FAILED"" critical_failures = sum( 1 for result in self.validation_results if result.severity == ValidationSeverity.CRITICAL and not result.status ) if critical_failures > 0: overall_status = ""CRITICAL"" report = { ""source"": source, ""date"": date, ""timestamp"": datetime.now().isoformat(), ""overall_status"": overall_status, ""summary"": { ""total_checks"": total_checks, ""passed_checks"": passed_checks, ""failed_checks"": failed_checks, ""success_rate"": (passed_checks / total_checks * 100) if total_checks > 0 else 0 }, ""severity_breakdown"": severity_counts, ""detailed_results"": [ { ""check_name"": result.check_name, ""severity"": result.severity.value, ""status"": ""PASSED"" if result.status else ""FAILED"", ""message"": result.message, ""details"": result.details, ""timestamp"": result.timestamp.isoformat(), ""record_count"": result.record_count } for result in self.validation_results ] } return report def _send_validation_metrics(self, source: str, report: Dict[str, Any]): \"\"\"Send validation metrics to CloudWatch.\"\"" try: metrics = [ { 'MetricName': 'ValidationSuccessRate', 'Value': report['summary']['success_rate'], 'Unit': 'Percent', 'Dimensions': [{'Name': 'Source', 'Value': source}] }, { 'MetricName': 'ValidationChecksTotal', 'Value': report['summary']['total_checks'], 'Unit': 'Count', 'Dimensions': [{'Name': 'Source', 'Value': source}] }, { 'MetricName': 'ValidationChecksFailed', 'Value': report['summary']['failed_checks'], 'Unit': 'Count', 'Dimensions': [{'Name': 'Source', 'Value': source}] } ] self.cloudwatch.put_metric_data( Namespace='CloudFlare/Validation', MetricData=metrics ) logger.info(f""Sent validation metrics for {source}"") except Exception as e: logger.error(f""Failed to send validation metrics: {e}"") def _get_historical_average(self, source: str) -> Optional[int]: \"\"\"Get historical average record count for comparison.\"\"\" try: # Query CloudWatch for historical data response = self.cloudwatch.get_metric_statistics( Namespace='CloudFlare/Processing', MetricName='RecordsProcessed', Dimensions=[{'Name': 'Source', 'Value': source}], StartTime=datetime.now() - timedelta(days=30), EndTime=datetime.now(), Period=86400, # Daily Statistics=['Average'] ) if response['Datapoints']: return int(sum(dp['Average'] for dp in response['Datapoints']) / len(response['Datapoints'])) except Exception as e: logger.warning(f""Could not retrieve historical data: {e}"") return None ``` **2. Validation Configuration (validation_config.yaml):** ```yaml validation: schema: required_columns: - Datetime - ClientIP - ClientRequestHost - Action - ClientCountry - Source - RiskScore - ProcessedAt column_types: Datetime: datetime ClientIP: string ClientRequestHost: string Action: string ClientCountry: string Source: string RiskScore: integer ProcessedAt: datetime data_quality: null_value_thresholds: ClientCountry: 10.0 # Allow 10% null values ClientRequestHost: 5.0 default: 1.0 # Default 1% threshold duplicate_threshold: 1.0 # Allow 1% duplicates business_logic: valid_actions: - ALLOW - BLOCK - CHALLENGE - LOG risk_score_range: min: 0 max: 100 ip_address_validation: enabled: true format_regex: '^(?:[0-9]{1,3}\.){3}[0-9]{1,3}$' statistics: anomaly_detection: enabled: true record_count_deviation_threshold: 50.0 # 50% deviation block_rate_threshold: 20.0 # Alert if >20% blocks historical_comparison_days: 30 cross_reference: record_count_variance_threshold: 5.0 # Allow 5% variance reporting: output_formats: - json - html - csv s3_bucket: cars-cloudflare-processed s3_prefix: validation/reports/ retention_days: 90 ``` **3. Validation CLI Interface:** ```python # validation_cli.py import click import polars as pl from cloudflare_processor.core.validator import CloudFlareDataValidator import yaml @click.command() @click.option('--source', required=True, help='Data source (2.ford, 3, 47)') @click.option('--date', required=True, help='Processing date (YYYYMMDD)') @click.option('--input-file', required=True, help='Path to processed parquet file') @click.option('--config', default='config/validation_config.yaml', help='Validation configuration file') @click.option('--output-dir', default='/data/validation', help='Output directory for reports') @click.option('--source-metadata', help='Path to source metadata JSON file') def validate_data(source, date, input_file, config, output_dir, source_metadata): \"\"\"Validate processed CloudFlare data.\"\"" # Load configuration with open(config, 'r') as f: config_data = yaml.safe_load(f) # Load processed data df = pl.read_parquet(input_file) # Load source metadata if provided metadata = None if source_metadata: with open(source_metadata, 'r') as f: metadata = json.load(f) # Initialize validator validator = CloudFlareDataValidator(config_data) # Run validation report = validator.validate_processed_data(df, source, date, metadata) # Save validation report os.makedirs(output_dir, exist_ok=True) report_file = f""{output_dir}/validation_report_{source}_{date}.json"" with open(report_file, 'w') as f: json.dump(report, f, indent=2) click.echo(f""Validation completed. Report saved to {report_file}"") click.echo(f""Overall status: {report['overall_status']}"") click.echo(f""Success rate: {report['summary']['success_rate']:.1f}%"") if __name__ == '__main__': validate_data() ``` **ACCEPTANCE CRITERIA:** - [ ] Comprehensive validation framework implemented with configurable rules - [ ] Schema validation for all required columns and data types - [ ] Data quality validation (null values, duplicates, ranges) - [ ] Business logic validation (IP formats, valid actions, risk scores) - [ ] Statistical validation with anomaly detection - [ ] Cross-reference validation with source data - [ ] Automated validation reporting in multiple formats - [ ] CloudWatch metrics integration for validation results - [ ] CLI interface for manual validation runs - [ ] Configuration-driven validation rules - [ ] Historical comparison capabilities - [ ] Integration with processing pipeline - [ ] Validation report storage and retention - [ ] Alert integration for critical validation failures **VALIDATION METRICS:** - Validation Success Rate (target: >95%) - Critical Validation Failures (target: 0) - Data Quality Score (target: >90%) - Record Count Variance (target: <5%) - Schema Compliance (target: 100%) **TESTING REQUIREMENTS:** 1. **Unit Tests:** - Individual validation functions - Configuration parsing - Metric calculation accuracy 2. **Integration Tests:** - End-to-end validation pipeline - CloudWatch metrics delivery - Report generation and storage 3. **Scenario Tests:** - Various data quality issues - Schema violations - Statistical anomalies - Cross-reference mismatches",CloudFlare Data Processing Migration to AWS,High,Development,data-validation,8,,,
